import os
import logging
import json
from base64 import b64decode
from Crypto.Cipher import AES
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def unpad(data):
    padding_length = data[-1]
    return data[:-padding_length]

def decrypt(data, key):
    cipher = AES.new(b64decode(key), AES.MODE_ECB)
    decrypted_data = cipher.decrypt(b64decode(data))
    return unpad(decrypted_data).decode()

def get_openai_client(model_instance_name):
    try:
        aes_key_base64 = os.environ['AES_KEY']
        encrypted_api_base = os.environ['ENCRYPTED_API_BASE']
        encrypted_api_key = os.environ['ENCRYPTED_API_KEY']
        api_version = os.environ['AZURE_API_VERSION']

        decrypted_api_base = decrypt(encrypted_api_base, aes_key_base64)
        decrypted_api_key = decrypt(encrypted_api_key, aes_key_base64)

        if not decrypted_api_base.endswith('/'):
            decrypted_api_base += '/'

        logger.info(f"Using OpenAI base: {decrypted_api_base}")

        return AzureChatOpenAI(
            deployment_name=model_instance_name,
            openai_api_base=decrypted_api_base,
            openai_api_key=decrypted_api_key,
            openai_api_version=api_version
        )
    except Exception as e:
        logger.error(f"Error initializing OpenAI client: {e}")
        raise

def validate_raml_file(file_content):
    if not file_content.strip().startswith("#%RAML"):
        raise ValueError("Invalid RAML file. Expected file to start with '#%RAML'.")

def lambda_handler(event, context):
    logger.info(f"Received event: {json.dumps(event)}")

    try:
        if 'body' not in event or not event.get('isBase64Encoded', False):
            raise ValueError("Binary file content is missing or not base64-encoded.")

        file_content = b64decode(event['body']).decode('utf-8')

        # Query param: input=sampleforraml
        query_params = event.get('queryStringParameters', {})
        input_param = query_params.get('input')
        if not input_param:
            raise ValueError("Missing 'input' query parameter.")

        if input_param != "sampleforraml":
            raise ValueError(f"Unsupported input value: {input_param}. Expected 'sampleforraml'.")

        # Validate it's a RAML file
        validate_raml_file(file_content)

        # Header param: model_instance_name
        model_instance_name = event['headers'].get('model_instance_name')
        if not model_instance_name:
            raise ValueError("Missing 'model_instance_name' in headers.")

        # Instructions and prompt
        instructions = (
            "You are an expert in analysing the RAML and generating the payload from the RAML. "
            "I am providing the RAML which will be used as an asset for designing my API. "
            "Please anlayze the RAML and generate the sample payload which will honour all the rules inside the RAML. "
            "You can refer the link for any doubts related to RAML https://raml.org/developers/raml-100-tutorial"
        )
        prompt = (
            "Please generate the payloads for all the endpoint in the RAML.\n\n"
            "Only return the raw JSON array or object. Do not include the words 'result', 'json', 'output', or any other labels."
        )

        full_prompt = f"{instructions}\n\n{prompt}\n\n{file_content}"

        # Call OpenAI
        client = get_openai_client(model_instance_name)
        response = client.invoke([HumanMessage(content=full_prompt)])
        result = response.content

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": result
        }

    except Exception as e:
        logger.error(f"An error occurred: {e}")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
