import gradio as gr
import mysql.connector
import json
from langchain_openai import AzureChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain_core.tracers.stdout import ConsoleCallbackHandler


# -------------------------------
# Configure Azure OpenAI
# -------------------------------
llm = AzureChatOpenAI(
    deployment_name="gpt-4_complex_conversions",
    azure_endpoint="https://testopenaiassets.openai.azure.com",
    openai_api_key="",  # üîë add your API key here
    openai_api_version="2025-01-01-preview"
)


# -------------------------------
# Configure MySQL
# -------------------------------
config = {
    "host": "localhost",
    "user": "root",
    "password": "Admin",
    "database": "test_data"
}


def fetch_logs():
    """Fetch logs or API performance data from MySQL."""
    conn = mysql.connector.connect(**config)
    cursor = conn.cursor(dictionary=True)

    try:
        # üëá Adjust table/columns here if needed
        cursor.execute("SELECT * FROM api_logs ORDER BY id DESC LIMIT 100")
        results = cursor.fetchall()
    except Exception as e:
        results = [{"error": str(e)}]

    cursor.close()
    conn.close()
    return results


# -------------------------------
# Chat Function
# -------------------------------
def chat_with_agent(user_prompt, history):
    try:
        # Fetch logs from DB
        results = fetch_logs()

        if not results or "error" in results[0]:
            return f"‚ö†Ô∏è Could not fetch logs from DB: {results}"

        # Create log summary (count per level)
        level_counts = {}
        for log in results:
            level = log.get("level", "UNKNOWN").upper()
            level_counts[level] = level_counts.get(level, 0) + 1

        summary = "\n".join([f"{level}: {count}" for level, count in level_counts.items()])
        debug_count = level_counts.get("DEBUG", 0)

        # Convert logs to JSON string (so LLM sees actual log rows)
        logs_str = json.dumps(results, indent=2, default=str)

        # Construct final prompt for LLM
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a senior data analyst. Analyze API/system logs. "
                    "You will be asked about log counts, debug frequency, errors, anomalies, etc. "
                    "Always use the provided logs to answer. If information is not in logs, say so."
                ),
            },
            {
                "role": "user",
                "content": (
                    f"{user_prompt}\n\n"
                    f"--- Log Summary ---\n{summary}\n\n"
                    f"Total DEBUG logs: {debug_count}\n\n"
                    f"--- Raw Logs (latest 100 rows) ---\n{logs_str}"
                )
            },
        ]

        # Call Azure LLM with callback to track token usage
        with get_openai_callback() as cb:
            result = llm.invoke(messages, config={"callbacks": [ConsoleCallbackHandler()]})
            final_response = result.content

            # Append token usage details
            final_response += "\n\n--- Token Usage ---"
            final_response += f"\nTotal Tokens: {cb.total_tokens}"
            final_response += f"\nPrompt Tokens: {cb.prompt_tokens}"
            final_response += f"\nCompletion Tokens: {cb.completion_tokens}"
            final_response += f"\nTotal Cost (USD): ${format(cb.total_cost, '.6f')}"

        return final_response

    except Exception as e:
        return f"‚ùå Error: {str(e)}"


# -------------------------------
# Gradio UI
# -------------------------------
chatbot_ui = gr.ChatInterface(
    fn=chat_with_agent,
    title="OSIF Co-Developer",
    description="Query system/API logs with natural language",
    theme="default"
)

# Launch UI
if __name__ == "__main__":
    chatbot_ui.launch(debug=False)