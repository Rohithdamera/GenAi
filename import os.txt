

import os
import json
import logging
from base64 import b64decode
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

logging.basicConfig(level=logging.INFO)

def unpad(data):
    pad_len = data[-1]
    return data[:-pad_len]

def decrypt(data, key_base64):
    key = b64decode(key_base64)
    cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
    decryptor = cipher.decryptor()
    decrypted_data = decryptor.update(b64decode(data)) + decryptor.finalize()
    return unpad(decrypted_data).decode()

def get_openai_client(model_name):
    try:
        aes_key = os.environ['AES_KEY']
        decrypted_base = decrypt(os.environ['ENCRYPTED_API_BASE'], aes_key)
        decrypted_key = decrypt(os.environ['ENCRYPTED_API_KEY'], aes_key)
        api_version = os.environ['AZURE_API_VERSION']

        if not decrypted_base.endswith('/'):
            decrypted_base += '/'

        return AzureChatOpenAI(
            deployment_name=model_name,
            openai_api_base=decrypted_base,
            openai_api_key=decrypted_key,
            openai_api_version=api_version,
            temperature=0.0,
            max_tokens=4096,
            model_kwargs={"top_p": 0.95}
        )
    except Exception as e:
        logging.error(f"OpenAI client init failed: {e}")
        raise

def detect_file_type(file_content):
    content = file_content.lower()
    if '#%raml' in content:
        return 'raml'
    elif '"swagger"' in content or '"openapi"' in content:
        return 'swagger'
    else:
        return 'unknown'

def generate_test_prompt(file_type, content):
    if file_type == 'raml':
        prompt = (
            "You are given a RAML API definition.\n"
            "Generate realistic test cases including endpoint names, HTTP methods, parameters, and sample request/response bodies.\n"
            "The output should be structured and readable, but do not include markdown syntax."
        )
    elif file_type == 'swagger':
        prompt = (
            "You are given a Swagger/OpenAPI specification in JSON format.\n"
            "Generate example test cases covering each endpoint with HTTP methods, example request and response payloads, headers, and status codes.\n"
            "Do not use markdown syntax, just plain text with structured formatting."
        )
    else:
        raise ValueError("Unsupported API file format. Only RAML or Swagger/OpenAPI JSON is supported.")

    return [
        SystemMessage(content=prompt),
        HumanMessage(content=content)
    ]

def lambda_handler(event, context):
    try:
        if 'body' not in event or not event['body']:
            raise ValueError("Missing request body.")

        if event.get('isBase64Encoded', False):
            file_content = b64decode(event['body']).decode('utf-8')
        else:
            file_content = event['body']

        file_type = detect_file_type(file_content)
        if file_type == 'unknown':
            raise ValueError("Unsupported or unrecognized file format. Only RAML or Swagger JSON supported.")

        model_name = event.get('headers', {}).get('model_instance_name', 'Default_Model')
        client = get_openai_client(model_name)
        messages = generate_test_prompt(file_type, file_content)
        response = client.invoke(messages)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "text/plain"},
            "body": response.content.strip()
        }

    except Exception as e:
        logging.error(f"Error: {e}")
        return {
            "statusCode": 400,
            "body": json.dumps({"error": str(e)})
        }
