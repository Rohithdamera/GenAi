import asyncio
import json
import nest_asyncio
from tabulate import tabulate
from mcp import ClientSession
from mcp.client.sse import sse_client
from langchain.chat_models import AzureChatOpenAI
from langgraph.graph import StateGraph

nest_asyncio.apply()

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",
        temperature=0.7,
        max_tokens=2000,
        model_kwargs={"top_p": 0.9, "frequency_penalty": 0.2, "presence_penalty": 0.1}
    )

sse_url = "https://employee-mcp-v1-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

# === Fetch Available Tools ===
def sync_fetch_tools():
    async def fetch():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.list_tools()
    return asyncio.get_event_loop().run_until_complete(fetch())

# === Call Tool ===
def sync_call_tool(tool_name: str, tool_input: dict):
    async def call():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.call_tool(tool_name, tool_input)
    try:
        return asyncio.get_event_loop().run_until_complete(call())
    except Exception as e:
        return f"[ERROR] {e}"

# === LangGraph Nodes ===
def list_tools_node(state: dict) -> dict:
    try:
        result = sync_fetch_tools()
        tools = [{
            "name": t.name,
            "description": t.description,
            "inputSchema": t.inputSchema
        } for t in getattr(result, "tools", [])]
        state["available_tools"] = tools
        state["__next__"] = "llm_agent"
        return state
    except Exception as e:
        return {"result": f"[ERROR] Failed to fetch tools: {e}"}

# === Agent Node ===
def llm_agent_node(state: dict) -> dict:
    llm = get_openai_client()
    tools = state.get("available_tools", [])
    tool_list = "\n".join(f"- {t['name']}: {t['description']}" for t in tools)

    agent_prompt = f"""
You are a smart assistant that knows how to select, combine, and call the right tools to answer a user query.
Here is the list of tools:

{tool_list}

Your task is to:
- Decide which tool(s) to use
- Extract correct inputs from the user query
- Call each tool using appropriate inputs
- Process and merge data from tool responses if needed
- Return a final answer to the user in natural language

If no data found or query doesn't match any tool, respond with a friendly message.
Return your final answer.

User query:
"{state['user_input']}"
"""

    try:
        response = llm.invoke(agent_prompt).content.strip()
        state["result"] = response
        state["__next__"] = "final_response"
        return state
    except Exception as e:
        return {"result": f"[ERROR] Agent failed to process: {e}"}

# === Final Response Node ===
def final_response_node(state: dict) -> dict:
    print("\nResponse:\n", state.get("result", "[No response]"))
    return state

# === Default Fallback Node ===
def default_node(state: dict) -> dict:
    return {"result": "Sorry, I couldn't understand your request or no matching tool found."}

# === Build LangGraph ===
def build_graph():
    graph = StateGraph(dict)
    graph.add_node("list_tools", list_tools_node)
    graph.add_node("llm_agent", llm_agent_node)
    graph.add_node("final_response", final_response_node)
    graph.add_node("default", default_node)

    graph.add_conditional_edges("list_tools", lambda s: s["__next__"], {
        "llm_agent": "llm_agent"
    })
    graph.add_conditional_edges("llm_agent", lambda s: s["__next__"], {
        "final_response": "final_response"
    })

    graph.set_entry_point("list_tools")
    return graph.compile()

# === Main Loop ===
if __name__ == "__main__":
    print("\n=== MCP Tool Assistant ===\nType 'exit' to quit.")
    tools_state = list_tools_node({})

    if "available_tools" not in tools_state:
        print(tools_state.get("result", "[ERROR] Could not load tools."))
        exit()

    runnable = build_graph()

    while True:
        user_input = input("\nAsk your question: ").strip()
        if user_input.lower() in ("exit", "quit"):
            print("Goodbye!")
            break

        tools_state.update({
            "user_input": user_input,
            "__next__": "llm_agent"
        })

        result = runnable.invoke(tools_state)
