import asyncio
import json
import nest_asyncio
from tabulate import tabulate

from langgraph.graph import StateGraph
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
import aiohttp

nest_asyncio.apply()

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",  # Fill in your key
        temperature=0.7,
        max_tokens=2000,
        model_kwargs={"top_p": 0.9, "frequency_penalty": 0.2, "presence_penalty": 0.1}
    )

# === SSE URL ===
sse_url = "https://employee-mcp-v1-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

# === Tool Call Function ===
async def call_tool(tool_name, tool_input):
    payload = {
        "method": "tools/call",
        "params": {
            "name": tool_name,
            "arguments": tool_input
        }
    }
    async with aiohttp.ClientSession() as session:
        async with session.post(sse_url, json=payload) as resp:
            return await resp.json()

# === List Tools Function ===
async def list_tools():
    async with aiohttp.ClientSession() as session:
        async with session.get(sse_url) as resp:
            return await resp.json()

# === LangGraph Nodes ===
def list_tools_node(state: dict) -> dict:
    tools = asyncio.run(list_tools()).get("tools", [])
    state["available_tools"] = tools
    state["__next__"] = "user_query"
    return state

def user_query_node(state: dict) -> dict:
    user_input = input("\nAsk your question (or type 'exit' to quit): ")
    if user_input.lower().strip() == "exit":
        return {"exit": True}
    state["user_input"] = user_input
    state["__next__"] = "decide"
    return state

def decide_node(state: dict) -> dict:
    tools = state["available_tools"]
    tool_list = "\n".join(f"- {t['name']}: {t['description']}" for t in tools)
    prompt = f"""
Given the tools:
{tool_list}

Decide which tools to call and with what input based on:
"{state['user_input']}"

Respond with a JSON list like:
[
  {{"tool": "ToolName", "input": {{...}}}},
  ...
]
If no relevant data, reply with [NO DATA FOUND]
"""
    try:
        llm = get_openai_client()
        response = llm.invoke(HumanMessage(content=prompt)).content.strip()
        if "[NO DATA FOUND]" in response:
            return {"result": "No relevant data found.", "__next__": "user_query"}
        state["tool_calls"] = json.loads(response)
        state["__next__"] = "execute"
        return state
    except Exception as e:
        return {"result": f"[ERROR parsing LLM decision]: {e}", "__next__": "user_query"}

def execute_node(state: dict) -> dict:
    results = []
    for call in state.get("tool_calls", []):
        tool = call.get("tool")
        args = call.get("input", {})
        try:
            output = asyncio.run(call_tool(tool, args))
            results.append({"tool": tool, "result": output})
        except Exception as e:
            results.append({"tool": tool, "error": str(e)})
    state["results"] = results
    state["__next__"] = "respond"
    return state

def respond_node(state: dict) -> dict:
    prompt = f"""
User asked: {state['user_input']}
Tool results:
{json.dumps(state['results'], indent=2)}

Give a clear and helpful response to the user.
"""
    llm = get_openai_client()
    answer = llm.invoke(HumanMessage(content=prompt)).content
    print("\nResponse:\n", answer)
    state["__next__"] = "user_query"
    return state

def exit_node(state: dict) -> dict:
    print("\n[INFO] Exiting. Goodbye!")
    return state

# === Build LangGraph ===
def build_graph():
    graph = StateGraph(dict)
    graph.add_node("list_tools", list_tools_node)
    graph.add_node("user_query", user_query_node)
    graph.add_node("decide", decide_node)
    graph.add_node("execute", execute_node)
    graph.add_node("respond", respond_node)
    graph.add_node("exit", exit_node)

    graph.add_conditional_edges("list_tools", lambda s: s["__next__"], {"user_query": "user_query"})
    graph.add_conditional_edges("user_query", lambda s: "exit" if s.get("exit") else s["__next__"], {
        "decide": "decide", "exit": "exit"
    })
    graph.add_conditional_edges("decide", lambda s: s["__next__"], {"execute": "execute", "user_query": "user_query"})
    graph.add_conditional_edges("execute", lambda s: s["__next__"], {"respond": "respond"})
    graph.add_conditional_edges("respond", lambda s: s["__next__"], {"user_query": "user_query"})

    graph.set_entry_point("list_tools")
    return graph.compile()

# === Main ===
if __name__ == "__main__":
    state = {}
    graph = build_graph()
    while True:
        state = graph.invoke(state)
        if state.get("exit"):
            break
