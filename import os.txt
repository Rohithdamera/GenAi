import urllib3
import json
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.agents import Tool, initialize_agent
from langchain.memory import ConversationBufferMemory

# === MCP Server Setup ===
MCP_URL = "https://your.mcp.server.url"  # Replace with your actual URL
MCP_HEADERS = {"Content-Type": "application/json"}
MCP_PAYLOAD = {
    "method": "tools/call",
    "params": {
        "name": "get-vendors",
        "arguments": {}
    }
}

mcp_raw_data = ""

# === Azure OpenAI Config ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        deployment_name="Fourth_Chatbot",
        openai_api_key="",  # Insert your Azure key
        openai_api_version="2024-08-01-preview",
        openai_api_type="azure",
        temperature=0.5,
        max_tokens=1500,
        model_kwargs={"top_p": 0.9}
    )

# === TOOL 1: MCP Raw Streaming Fetch ===
def fetch_and_store_mcp_data(_: str) -> str:
    global mcp_raw_data

    try:
        print("‚è≥ Connecting to MCP server using raw stream...")

        http = urllib3.PoolManager(timeout=urllib3.util.Timeout(connect=30, read=300))

        encoded_body = json.dumps(MCP_PAYLOAD).encode("utf-8")
        response = http.request(
            "POST",
            MCP_URL,
            headers=MCP_HEADERS,
            body=encoded_body,
            preload_content=False  # <== KEY: stream the response
        )

        raw_chunks = b""
        for chunk in response.stream(amt=4096):
            raw_chunks += chunk

        response.release_conn()
        raw_text = raw_chunks.decode("utf-8")
        parsed_json = json.loads(raw_text)
        mcp_raw_data = json.dumps(parsed_json, indent=2)

        return "[SUCCESS] MCP data fetched and stored successfully."
    except Exception as e:
        return f"[ERROR] MCP stream fetch failed: {str(e)}"

# === TOOL 2: Summarize MCP JSON ===
def summarize_mcp_data(_: str) -> str:
    global mcp_raw_data
    if not mcp_raw_data:
        return "[SKIP] MCP data not available."

    try:
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split_text(mcp_raw_data)

        llm = get_openai_client()
        prompt = PromptTemplate(
            input_variables=["chunk"],
            template="""
You are an assistant. Summarize the following vendor JSON in plain English:
- Vendor names
- Key properties
- Any unique configs or fields

Chunk:
{chunk}
"""
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        summaries = [chain.run({"chunk": c}) for c in chunks]
        return "\n\n".join(summaries)
    except Exception as e:
        return f"[ERROR] Failed to summarize: {str(e)}"

# === LangChain Agent Setup ===
def run_agent():
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    tools = [
        Tool(name="FetchMCPData", func=fetch_and_store_mcp_data, description="Fetch from MCP using raw streaming."),
        Tool(name="SummarizeMCPData", func=summarize_mcp_data, description="Summarize vendor response.")
    ]
    return initialize_agent(
        tools=tools,
        llm=get_openai_client(),
        agent_type="openai-functions",
        memory=memory,
        verbose=True
    )

# === MAIN ===
if __name__ == "__main__":
    print("üîß Starting MCP Agent with streaming...")
    agent = run_agent()
    result = agent.invoke("Fetch vendor data from MCP and summarize it.")
    print("\n‚úÖ Final Summary Output:\n")
    print(result)
