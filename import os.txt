import os
import shutil
import zipfile
import json
from base64 import b64decode
import logging

import openai  # Ensure openai package is included in Lambda layer if you're deploying this

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def unzip_raml(zip_bytes, extract_dir="/tmp/temp_raml"):
    if os.path.exists(extract_dir):
        shutil.rmtree(extract_dir)
    os.makedirs(extract_dir, exist_ok=True)

    zip_path = "/tmp/uploaded.zip"
    with open(zip_path, "wb") as f:
        f.write(zip_bytes)

    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    return extract_dir

def find_main_raml_file(base_dir):
    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".raml"):
                full_path = os.path.join(root, file)
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                    if "#%RAML 1.0" in f.readline():
                        return full_path
    raise FileNotFoundError("Main RAML file not found.")

def resolve_include_path(include_path, root_dir, current_dir):
    possible_paths = [
        os.path.join(current_dir, include_path),
        os.path.join(root_dir, include_path)
    ]
    for path in possible_paths:
        if os.path.isfile(path):
            return path
    for root, _, files in os.walk(root_dir):
        for f in files:
            if f == os.path.basename(include_path):
                return os.path.join(root, f)
    return None

def resolve_includes_in_raml(file_path, root_dir):
    resolved_lines = []
    current_dir = os.path.dirname(file_path)

    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        for line in file:
            if "!include" in line:
                parts = line.strip().split("!include")
                prefix = parts[0].strip()
                include_path = parts[1].strip()
                include_file = resolve_include_path(include_path, root_dir, current_dir)
                if include_file and os.path.exists(include_file):
                    with open(include_file, 'r', encoding='utf-8', errors='ignore') as inc_file:
                        resolved_lines.append(f"{prefix} |\n")
                        for inc_line in inc_file.read().splitlines():
                            resolved_lines.append(f"  {inc_line}")
                else:
                    resolved_lines.append(f"{line.strip()}  # Include not found")
            else:
                resolved_lines.append(line.rstrip())

    return "\n".join(resolved_lines)

def analyze_raml_with_openai(model_name, resolved_raml_content):
    prompt = """
You are an expert RAML analyst and realistic test data generator.

You are provided with the full contents of a RAML project, including all resolved includes (traits, resource types, examples, etc.).

Your task:
- Analyze the RAML structure and for each endpoint, generate **two distinct and realistic JSON payloads**.
- Respect the data types of each field (e.g., integers should be integers, strings should be realistic random strings, booleans should be true/false).
- Do **not** reuse values from any examples in the RAML.
- Include all required fields; optional fields may be included randomly.
- All payloads must be valid JSON objects.
- Return a **single JSON object**, where each endpoint maps to a list of two valid and distinct payloads.

Now process the RAML content below and generate the test payloads:
"""

    messages = [
        {"role": "system", "content": "You are a RAML API expert and test data generator."},
        {"role": "user", "content": prompt + "\n\n" + resolved_raml_content}
    ]

    response = openai.ChatCompletion.create(
        model=model_name,
        messages=messages,
        temperature=0.7,
        max_tokens=2000
    )

    return response.choices[0].message["content"]

def lambda_handler(event, context):
    try:
        if 'body' not in event or not event.get('isBase64Encoded', False):
            raise ValueError("File content is missing or not base64-encoded.")

        zip_bytes = b64decode(event['body'])

        headers = event.get('headers', {})
        model_name = (
            headers.get('model_instance_name') or
            headers.get('Model_Instance_Name') or
            os.environ.get('DEFAULT_MODEL_INSTANCE_NAME', 'gpt-4')
        )

        openai.api_key = os.environ.get("OPENAI_API_KEY")  # required to be set in Lambda

        extracted_dir = unzip_raml(zip_bytes)
        main_raml_file = find_main_raml_file(extracted_dir)
        resolved_raml = resolve_includes_in_raml(main_raml_file, extracted_dir)

        final_output = analyze_raml_with_openai(model_name, resolved_raml)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": final_output
        }

    except Exception as e:
        logger.exception("Exception occurred:")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
