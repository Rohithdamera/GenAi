import json
import base64
import logging
import boto3
import random
import string

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Generates dummy value matching expected data type
def generate_dummy_value(example_value):
    if isinstance(example_value, int):
        return random.randint(1000, 9999)
    elif isinstance(example_value, float):
        return round(random.uniform(1.0, 9999.99), 2)
    elif isinstance(example_value, str):
        return ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5, 10)))
    elif isinstance(example_value, bool):
        return random.choice([True, False])
    elif isinstance(example_value, list):
        return [generate_dummy_value(example_value[0]) if example_value else "example"]  # Safe default
    elif isinstance(example_value, dict):
        return generate_dummy_payload(example_value)
    else:
        return "example"

# Recursively replace values in JSON structure
def generate_dummy_payload(data):
    if isinstance(data, dict):
        return {k: generate_dummy_payload(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [generate_dummy_payload(v) for v in data]
    else:
        return generate_dummy_value(data)

# Extract model instance name dynamically from headers
def get_model_instance_name(event):
    return event['headers'].get('model_instance_name', 'default-model')

# Call the GenAI model through Bedrock or OpenAI
def call_genai_model(model_instance, prompt, input_json):
    client = boto3.client('bedrock-runtime')  # Ensure your Lambda has Bedrock permissions

    body = {
        "prompt": f"{prompt}\n\n{json.dumps(input_json, indent=2)}\n\nReturn two unique randomized payloads as JSON.",
        "max_tokens_to_sample": 2048,
        "temperature": 0.7,
    }

    response = client.invoke_model(
        modelId=model_instance,
        body=json.dumps(body),
        contentType="application/json",
        accept="application/json"
    )

    result = json.loads(response['body'].read().decode())
    return json.loads(result['completion'])  # Expecting clean JSON string in completion

# Main Lambda handler
def lambda_handler(event, context):
    logger.info("Lambda function invoked.")
    try:
        if 'body' not in event or not event.get('isBase64Encoded', False):
            raise ValueError("Missing or invalid file data in request.")

        # Decode the incoming base64 content
        content = base64.b64decode(event['body']).decode('utf-8')
        json_start = content.find('{')
        if json_start == -1:
            raise ValueError("No JSON object found in RAML file.")

        json_data = json.loads(content[json_start:])

        # Dynamically extract the model instance
        model_instance = get_model_instance_name(event)

        # Prepare prompt for test data generation
        prompt = (
            "You are a RAML analysis expert. Based on the structure provided below, "
            "generate two randomized test payloads preserving the original field names and structure. "
            "Each value must be replaced with a randomized one matching the correct data type (int, float, string, boolean, etc.)."
        )

        # Call GenAI model dynamically
        result = call_genai_model(model_instance, prompt, json_data)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps(result)
        }

    except Exception as e:
        logger.error(f"Error in Lambda handler: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": str(e)})
        }
