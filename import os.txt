import os
import json
import logging
from base64 import b64decode
from Crypto.Cipher import AES
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

logging.basicConfig(level=logging.INFO)

def unpad(data: bytes) -> bytes:
    return data[:-data[-1]]

def decrypt(data: str, key: str) -> str:
    cipher = AES.new(b64decode(key), AES.MODE_ECB)
    return unpad(cipher.decrypt(b64decode(data))).decode()

def get_openai_client(model_instance_name: str) -> AzureChatOpenAI:
    aes_key = os.environ['AES_KEY']
    api_base = decrypt(os.environ['ENCRYPTED_API_BASE'], aes_key)
    api_key = decrypt(os.environ['ENCRYPTED_API_KEY'], aes_key)
    api_version = os.environ['AZURE_API_VERSION']

    if not api_base.endswith('/'):
        api_base += '/'

    return AzureChatOpenAI(
        deployment_name=model_instance_name,
        openai_api_base=api_base,
        openai_api_key=api_key,
        openai_api_version=api_version,
        temperature=0.3,
        max_tokens=2048,
        model_kwargs={
            "top_p": 0.9,
            "frequency_penalty": 0,
            "presence_penalty": 0
        }
    )

# Force output to be Mermaid only, no explanation
MERMAID_ONLY_PROMPT = (
    "You are an AI that strictly generates **only Mermaid diagrams** from BizTalk .odx or JSON config content.\n\n"
    "Do NOT write paragraphs, summaries, or descriptions.\n"
    "Only respond using the :::mermaid code block.\n"
    "Start with :::mermaid and end with :::.\n"
    "Use flowchart TD format.\n"
    "Make sure the diagram shows all steps and transitions clearly.\n"
    "Example format:\n"
    ":::mermaid\n"
    "flowchart TD\n"
    "    Step1[Step 1]\n"
    "    Step2[Step 2]\n"
    "    Step1 --> Step2\n"
    ":::\n"
)

def process_with_openai(client: AzureChatOpenAI, file_text: str, prompt: str) -> str:
    messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=file_text)
    ]
    result = client.invoke(messages).content.strip()
    # Extract only :::mermaid block
    if ":::mermaid" in result and ":::” in result:
        start = result.find(":::mermaid")
        end = result.find(":::", start + 10)
        mermaid_block = result[start:end+3]
        return mermaid_block
    return result  # fallback

def detect_file_type_and_decode(raw_body: str) -> str:
    try:
        return raw_body.decode('utf-8')
    except Exception:
        raise ValueError("Uploaded file must be a valid UTF-8 encoded .odx or .json file")

def lambda_handler(event: dict, context: dict) -> dict:
    try:
        raw_body = event.get('body')
        if not raw_body:
            raise ValueError("Missing request body.")

        if event.get("isBase64Encoded", False):
            raw_body = b64decode(raw_body)
        else:
            raw_body = raw_body.encode('utf-8')

        headers = event.get('headers', {})
        model_instance_name = headers.get('model_instance_name')
        if not model_instance_name:
            raise ValueError("Missing 'model_instance_name' in headers.")

        # Use custom prompt only if absolutely needed
        custom_prompt = headers.get('prompt', '').strip()
        final_prompt = custom_prompt if custom_prompt else MERMAID_ONLY_PROMPT

        file_text = detect_file_type_and_decode(raw_body)
        client = get_openai_client(model_instance_name)
        result = process_with_openai(client, file_text, final_prompt)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"output": result})
        }

    except Exception as e:
        logging.error(f"Error: {e}")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
