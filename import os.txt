import os
import json
import logging
import random
import string
from base64 import b64decode

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ========== MOCK OPENAI CLIENT ==========

def mock_openai_client(model_instance_name):
    class MockClient:
        def invoke(self, messages):
            raml_content = messages[0]['content']
            endpoints = extract_endpoints_from_raml(raml_content)
            payloads = generate_randomized_payloads(endpoints, raml_content)
            return type('obj', (object,), {'content': json.dumps(payloads)})
    return MockClient()

# ========== PAYLOAD GENERATION LOGIC ==========

def extract_endpoints_from_raml(raml_content):
    endpoints = []
    for line in raml_content.splitlines():
        if line.startswith('/'):
            endpoints.append(line.strip())
    return endpoints

def generate_randomized_payloads(endpoints, raml_content):
    payloads = {}
    for endpoint in endpoints:
        payloads[endpoint] = extract_and_randomize_fields(endpoint, raml_content)
    return payloads

def extract_and_randomize_fields(endpoint, raml_content):
    fields = {}
    capture = False
    for line in raml_content.splitlines():
        if line.strip() == endpoint:
            capture = True
            continue
        if capture and line.startswith('/'):
            break
        if capture and ':' in line:
            key, value = map(str.strip, line.split(':', 1))
            if key:
                fields[key] = generate_dummy_value(value)
    return fields

# ========== DUMMY VALUE GENERATOR ==========

def generate_dummy_value(original_value):
    if original_value.isdigit():
        return random.randint(1000, 9999)
    if original_value.replace('.', '', 1).isdigit():
        return round(random.uniform(10.0, 9999.99), 2)
    if original_value.lower() in ['true', 'false']:
        return random.choice([True, False])
    if original_value.startswith('"') or original_value.startswith("'"):
        return ''.join(random.choices(string.ascii_letters + string.digits, k=8))
    return ''.join(random.choices(string.ascii_letters + string.digits, k=10))

# ========== OPENAI WRAPPER ==========

def analyze_raml_with_openai(client, raml_content):
    try:
        prompt = (
            "You are analyzing a RAML file with multiple endpoints and fields. "
            "Your job is to retain all the field names exactly as-is, "
            "but replace each field value with a randomized dummy value. "
            "Ensure the structure and endpoint definitions are preserved. "
        )
        response = client.invoke([{"content": prompt + "\n\n" + raml_content}])
        return response.content
    except Exception as e:
        logger.error(f"Error analyzing RAML with OpenAI: {e}")
        raise ValueError(f"Error analyzing RAML with OpenAI: {e}")

# ========== MAIN LAMBDA HANDLER ==========

def lambda_handler(event, context):
    logger.info("Lambda invoked.")
    try:
        if 'body' not in event or not event.get('isBase64Encoded', False):
            raise ValueError("RAML content is missing or not base64-encoded.")

        raml_bytes = b64decode(event['body'])
        raml_content = raml_bytes.decode('utf-8', errors='ignore')

        headers = event.get('headers', {})
        model_instance_name = (
            headers.get('model_instance_name') or
            headers.get('Model_Instance_Name') or
            os.environ.get('DEFAULT_MODEL_INSTANCE_NAME', 'default-model')
        )

        client = mock_openai_client(model_instance_name)
        final_output = analyze_raml_with_openai(client, raml_content)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": final_output
        }

    except Exception as e:
        logger.error(f"Error during execution: {e}")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
