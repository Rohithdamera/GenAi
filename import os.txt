import asyncio
import json
import nest_asyncio
import pandas as pd
import uuid
import os

from mcp import ClientSession
from mcp.client.sse import sse_client
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import Tool, initialize_agent
from langchain.agents.agent_types import AgentType
from langchain.memory import ConversationBufferMemory

nest_asyncio.apply()

# === Azure OpenAI Setup ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",  # Your key here
        temperature=0.3,
        max_tokens=2000,
    )

# === SSE Tool Call Setup ===
sse_url = "https://mcp-server-sse-cg-employees-sandbox-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

def sync_call_tool(tool_name: str, params: dict):
    async def call():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.call_tool(tool_name, params)
    return asyncio.get_event_loop().run_until_complete(call())

def fetch_all_tools():
    async def fetch():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.list_tools()
    return asyncio.get_event_loop().run_until_complete(fetch())

# === Tool Conversion for Agent ===
def get_tools():
    raw = fetch_all_tools()
    tools = []

    for t in getattr(raw, "tools", []):
        name = t.name
        required_fields = t.inputSchema.get("required", []) if t.inputSchema else []

        def make_func(tool_name=name, req=required_fields):
            def fn(input_str: str) -> str:
                try:
                    parsed = json.loads(input_str) if input_str else {}
                except:
                    parsed = {}
                for key in req:
                    parsed.setdefault(key, "")
                try:
                    result = sync_call_tool(tool_name, parsed)
                    return json.dumps(result, indent=2, default=str)
                except Exception as e:
                    return f"[ERROR] {tool_name}: {str(e)}"
            return fn

        tools.append(
            Tool(
                name=name,
                func=make_func(),
                description=f"{t.description or ''} Required fields: {required_fields}",
                return_direct=False,
            )
        )

    return tools

# === Output as Table ===
def render_output_as_table(output_text: str):
    try:
        data = json.loads(output_text)
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            df = pd.DataFrame([data])
        else:
            return output_text
        return df.to_string(index=False)
    except Exception:
        return output_text

# === Custom Prompt ===
CUSTOM_PREFIX = """
You are an intelligent support agent with access to multiple tools containing employee, project, and organizational data.

Your responsibilities:

- Always check all tools for relevant data before concluding that something is not found.
- If a tool returns partial information (like employee ID), use that to search in other tools.
- If names are misspelled or abbreviated (e.g., HYD for Hyderabad, benguluru for Bangalore), still attempt to resolve them using available data across all tools.
- Present the final answer in a tabular format without any emojis or markdown.
- Always attempt a multi-hop reasoning process until an accurate and complete answer is found.
- Never stop after a single failed attempt unless all tools are tried.

You must produce clear, structured, complete answers.
"""

# === Chat History Detection ===
def is_chat_history_query(query: str) -> bool:
    query = query.lower()
    return any(phrase in query for phrase in [
        "chat history", "show previous", "conversation summary", "previous questions", "summarize chat"
    ])

# === Summarize In-Memory History ===
def summarize_chat_history(memory) -> str:
    messages = memory.chat_memory.messages
    summary = []

    for i in range(0, len(messages), 2):
        user_msg = messages[i].content if i < len(messages) else ""
        ai_msg = messages[i + 1].content if i + 1 < len(messages) else ""
        summary.append({
            "Question": user_msg.strip(),
            "Answer": ai_msg.strip()
        })

    df = pd.DataFrame(summary)
    return df.to_string(index=False)

# === Build the Agent and Memory ===
def build_agent():
    tools = get_tools()
    llm = get_openai_client()
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        max_iterations=10,
        handle_parsing_errors=True,
        memory=memory,
        agent_kwargs={"prefix": CUSTOM_PREFIX}
    )
    conversation_id = str(uuid.uuid4())
    return agent, memory, conversation_id

# === Save Chat History to Disk ===
def save_chat_history(conversation_id, memory):
    os.makedirs("chat_logs", exist_ok=True)
    file_path = f"chat_logs/{conversation_id}.json"
    history_data = []
    for i in range(0, len(memory.chat_memory.messages), 2):
        user = memory.chat_memory.messages[i].content
        ai = memory.chat_memory.messages[i + 1].content if i + 1 < len(memory.chat_memory.messages) else ""
        history_data.append({"query": user, "response": ai})
    with open(file_path, "w") as f:
        json.dump(history_data, f, indent=2)
    print(f"\n[Chat history saved to {file_path}]")

# === CLI Interface ===
if __name__ == "__main__":
    print("=== MCP ReAct Agent (with Chat History Summary) ===")
    agent, memory, conversation_id = build_agent()

    print(f"\n[Session Conversation ID: {conversation_id}]")

    while True:
        query = input("\nYour question (or type 'exit'): ").strip()

        if query.lower() in ("exit", "quit"):
            save_chat_history(conversation_id, memory)
            break

        if is_chat_history_query(query):
            print("\nðŸ“œ Chat History Summary:\n")
            print(summarize_chat_history(memory))
            continue

        try:
            print(f"\n[Conversation ID: {conversation_id}]")
            result = agent.invoke({"input": query})
            final = result["output"]
            print("\nAnswer:\n")
            print(render_output_as_table(final))

        except Exception as e:
            print("Error:", str(e))
