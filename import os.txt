import asyncio
import json
import nest_asyncio
import pandas as pd
import uuid
import os
 
from mcp import ClientSession
from mcp.client.sse import sse_client
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import Tool, initialize_agent
from langchain.agents.agent_types import AgentType
 
nest_asyncio.apply()
 
# === Azure OpenAI Setup ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",
        temperature=0.3,
        max_tokens=2000,
    )
 
# === Tool Calling Logic ===
sse_url = "https://mcp-server-sse-cg-employees-sandbox-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"
 
def sync_call_tool(tool_name: str, params: dict):
    async def call():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.call_tool(tool_name, params)
    return asyncio.get_event_loop().run_until_complete(call())
 
def fetch_all_tools():
    async def fetch():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.list_tools()
    return asyncio.get_event_loop().run_until_complete(fetch())
 
# === Tool Conversion ===
def get_tools():
    raw = fetch_all_tools()
    tools = []
 
    for t in getattr(raw, "tools", []):
        name = t.name
        required_fields = t.inputSchema.get("required", []) if t.inputSchema else []
 
        def make_func(tool_name=name, req=required_fields):
            def fn(input_str: str) -> str:
                try:
                    parsed = json.loads(input_str) if input_str else {}
                except:
                    parsed = {}
                for key in req:
                    parsed.setdefault(key, "")
                try:
                    result = sync_call_tool(tool_name, parsed)
                    return json.dumps(result, indent=2, default=str)
                except Exception as e:
                    return f"[ERROR] {tool_name}: {str(e)}"
            return fn
 
        tools.append(
            Tool(
                name=name,
                func=make_func(),
                description=f"{t.description or ''} Required fields: {required_fields}",
                return_direct=False,
            )
        )
 
    return tools
 
# === Table Renderer ===
def render_output_as_table(output_text: str):
    try:
        data = json.loads(output_text)
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            df = pd.DataFrame([data])
        else:
            return output_text
        return df.to_string(index=False)
    except Exception:
        return output_text
 
# === Custom Prompt ===
CUSTOM_PREFIX = """
You are an intelligent support agent with access to multiple tools containing employee, project, and organizational data.
 
Your responsibilities:
 
- Always check all tools for relevant data before concluding that something is not found.
- If a tool returns partial information (like employee ID), use that to search in other tools.
- If names are misspelled or abbreviated (e.g., HYD for Hyderabad, benguluru for Bangalore), still attempt to resolve them using available data across all tools.
- Present the final answer in a **tabular format** without any emojis or markdown.
- Always attempt a multi-hop reasoning process until an accurate and complete answer is found.
- Never stop after a single failed attempt unless all tools are tried.
 
You must produce clear, structured, complete answers.
"""
 
# === Build the Agent ===
def build_agent():
    tools = get_tools()
    llm = get_openai_client()
    return initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        max_iterations=10,
        handle_parsing_errors=True,
        agent_kwargs={"prefix": CUSTOM_PREFIX}
    )
 
# === Save Chat History ===
def save_chat_history(conversation_id, chat_history):
    os.makedirs("chat_logs", exist_ok=True)
    file_path = f"chat_logs/{conversation_id}.json"
    with open(file_path, "w") as f:
        json.dump(chat_history, f, indent=2)
    print(f"\n[Chat history saved to {file_path}]")
 
# === CLI Interface ===
if __name__ == "__main__":
    print("=== MCP ReAct Agent (Deep Tool Search, Cross-linked) ===")
    agent = build_agent()
 
    conversation_id = str(uuid.uuid4())
    chat_history = []
 
    print(f"\n[Session Conversation ID: {conversation_id}]")
 
    while True:
        query = input("\nYour question: ").strip()
        if query.lower() in ("exit", "quit"):
            save_chat_history(conversation_id, chat_history)
            break
 
        try:
            print(f"\n[Conversation ID: {conversation_id}]")
            result = agent.invoke({"input": query})
            final = result["output"]
            print("\nAnswer:\n")
            print(render_output_as_table(final))
 
            # Store in chat history
            chat_history.append({
                "query": query,
                "response": final
            })
 
        except Exception as e:
            print(" Error:", str(e))
 


-----------------------------------------------------------------------------


agent2.invoke("Locate and summarize pom.xml, store internally for Agent 3.")
 
    print("\n=== Agent 3: Unified Summary Generator ===")
    memory3 = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    agent3 = initialize_agent(
        tools=[
            Tool(name="GenerateUnifiedSummary", func=generate_unified_report, description="Combine Mule and pom.xml summaries into one report."),
        ],
        llm=get_openai_client(),
        agent_type="openai-functions",
        memory=memory3,
        verbose=True
    )
    final_report = agent3.invoke("Generate final unified summary combining Mule XML and Maven pom.xml insights.")
    print("\n[FINAL UNIFIED SUMMARY]:\n")
    print(final_report)
