import os
import json
from pathlib import Path
import gradio as gr
import re
from typing import List, Dict
from langchain.prompts import PromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain.callbacks import get_openai_callback
 
# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        deployment_name="gpt-4_complex_conversions",
        openai_api_key="",  # Use your actual key or env var
        openai_api_version="2024-08-01-preview",
        openai_api_type="azure",
        temperature=0.3,
        max_tokens=2000,
    )
 
llm = get_openai_client()
 
# === Step 1: Find Java Files Recursively ===
def find_java_files(path: str) -> List[Path]:
    base_path = Path(path)
    if not base_path.exists():
        raise FileNotFoundError(f"Path not found: {path}")
    return list(base_path.rglob("*.java"))
 
# === Step 2: Extract Metadata from Java Files ===
def extract_class_metadata(java_paths: List[Path]) -> List[Dict]:
    """
    Also determines if a Service uses a Repository
    and extracts all entity fields for later test generation.
    """
    prompt = PromptTemplate(
        input_variables=["source"],
        template="""
# Analyze the following Java source code and return JSON with:
# class_name
# class_type (Controller, Service, Repository, Config, Entity, DTO, Main, etc.)
# package_path
# methods: list of public method names
# uses_repository: true if this class has any field or constructor parameter whose type name ends with 'Repository'
# if class_type == "Entity", also include:
#     entity_fields: list of objects with {{"name": fieldName, "type": fieldType}}
 
# Only return compact valid JSON. No explanation.
 
# Java Source:
 
Analyze the following Java source code and return JSON with strictly these keys:
- class_name
- class_type (must be exactly one of: "Controller", "Service", "Repository", "Config", "Entity", "DTO", "Main")
- package_path
- methods: list of all public method names
- uses_repository: true/false (true if this class has a field or constructor parameter ending with 'Repository')
 
If class_type == "Entity", also include:
  "entity_fields": list of objects like { "name": fieldName, "type": fieldType }
 
 Important:
- Do NOT invent new class_type values. Only pick from the allowed list above.
- Do NOT output explanations. Only valid compact JSON.
 
Java Source:
 
{source}
"""
    )
    chain = prompt | llm
    parsed = []
 
    for file_path in java_paths:
        try:
            source = file_path.read_text(encoding="utf-8")
            response = chain.invoke({"source": source})
            data = json.loads(response.content.strip())
            parsed.append(data)
        except Exception as e:
            print(f"[WARN] Failed to parse {file_path}: {e}")
    return parsed
 
# === Step 3: Generate JUnit Tests (only for Controller & Service) ===
def generate_junit_tests_from_metadata(parsed_info: List[Dict]) -> List[Dict]:
    ALLOWED_TYPES = {"Controller", "Service"}  # Only generate for these types
 
    # Build a dictionary of entity fields so that when we see them in Service/Controller params, we can populate them
    entity_map = {
        item["class_name"]: item.get("entity_fields", [])
        for item in parsed_info
        if item.get("class_type") == "Entity"
    }
 
    prompt = PromptTemplate(
        input_variables=["class_name", "class_type", "package_path", "methods", "uses_repository", "entity_map"],
        template="""
Write a full JUnit 5 test class or:
Class Name: {class_name}
Type: {class_type}
Package: {package_path}
Public Methods:
{methods}
uses_repository: {uses_repository}
Available Entities:
Also, you have access to the following Entity definitions with their fields:
{entity_map}
 

Rules:

1. **For Controllers**
   - Annotate the test class with `@WebMvcTest(<ControllerClass>.class)`.
   - Use `MockMvc` with `@Autowired`.
   - Use `@MockBean` for all dependent services.
   - Include `ObjectMapper` with `@Autowired` to handle JSON serialization.
   - In `@BeforeEach`, fully initialize a test entity/DTO object with **all fields** from entity_map.
   - For `POST`/`PUT`: mock service call with `when(service.method(...)).thenReturn(...)`, send request with `objectMapper.writeValueAsString(entity)`, and assert each JSON field using `jsonPath`.
   - For `GET by ID`: mock service call, then assert returned JSON fields.
   - For `GET all`: mock service call returning a list, then assert at least the first object fields in `jsonPath`.
   - Always check **all DTO/entity fields** in assertions, not just ID.

2. **For Services**
   - Annotate the test class with nothing special unless repository is used.  
   - If no repository is present, instantiate service directly in `@BeforeEach`.
   - If repository exists, use `@ExtendWith(MockitoExtension.class)`, mock repository with `@Mock`, and inject it into the service with `@InjectMocks`.
   - Always populate entities fully in `@BeforeEach` with dummy but realistic values for every field.
   - For each public method:
     - If it saves/adds an entity: assert that returned entity is not null and all fields match expectations.
     - If it fetches by ID: assert the result is not null and fields match.
     - If it fetches all: assert the result list is not empty and contains the entity.
   - Use `assertThat(...)` from AssertJ for all assertions.

3. **Entity/DTO Population**
   - Always include **all fields** from the entity_map.
   - Populate values based on type:
     - `String` → `"John Doe"`, `"cand123"`
     - `UUID` → `UUID.randomUUID().toString()`
     - `List<String>` → `Arrays.asList("Java","Spring","SQL")`
     - `LocalDateTime` or datetime string → `"2025-07-25T10:00:00"`
     - `int` → `123`
     - `boolean` → `true`

4. **General**
   - One `@Test` method per public method in the class.
   - For controllers, never hardcode JSON manually; always use `ObjectMapper`.
   - For services, never `verify(service)` — instead, use real instance behavior and assert outputs.
   - Always generate compilable, valid Java code only.

 
"""
 
 
    )
    chain = prompt | llm
 
    test_classes = []
    for item in parsed_info:
        if item["class_type"] not in ALLOWED_TYPES:
            print(f"[SKIP] Skipping {item['class_name']} ({item['class_type']})")
            continue
        try:
            result = chain.invoke({
                "class_name": item["class_name"],
                "class_type": item["class_type"],
                "package_path": item["package_path"],
                "methods": "\n".join(item["methods"]),
                "uses_repository": str(item.get("uses_repository", False)).lower(),
                "entity_map": json.dumps(entity_map, indent=2)
            })
            test_classes.append({
                "file_name": f"{item['class_name']}Test.java",
                "package_path": item["package_path"],
                "code": result.content.strip()
            })
        except Exception as e:
            print(f"[WARN] Could not generate test for {item['class_name']}: {e}")
    return test_classes
 
# === Step 4: Save Files ===
def save_and_format_output(junit_tests: List[Dict]) -> str:
    output = ""
    for test in junit_tests:
        folder = Path("generated_tests") / test["package_path"].replace(".", "/")
        folder.mkdir(parents=True, exist_ok=True)
        file_path = folder / test["file_name"]
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(test["code"])
        output += f"\n===== {test['file_name']} =====\n{test['code']}\n===== End of {test['file_name']} =====\n"
    return output.strip()
 
# === Main Function ===
def process_java_project(prompt: str, history):
    aggregated_token_usage = {
        "total_tokens": 0,
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_cost": 0.0
    }
 
    try:
        path_match = re.search(r"([A-Za-z]:[\\/\w\-. ]+)", prompt)
        if not path_match:
            return "[ERROR] Please provide a valid Windows file path."
 
        project_path = path_match.group(1).strip()
        java_paths = find_java_files(project_path)
        if not java_paths:
            return "[INFO] No Java files found."
 
        with get_openai_callback() as cb:
            parsed_info = extract_class_metadata(java_paths)
            if not parsed_info:
                return "[INFO] No parsable Java classes found."
 
            test_classes = generate_junit_tests_from_metadata(parsed_info)
            if not test_classes:
                return "[INFO] No JUnit tests could be generated for allowed types."
 
            output_code = save_and_format_output(test_classes)
 
            aggregated_token_usage["total_tokens"] += cb.total_tokens
            aggregated_token_usage["prompt_tokens"] += cb.prompt_tokens
            aggregated_token_usage["completion_tokens"] += cb.completion_tokens
            aggregated_token_usage["total_cost"] += cb.total_cost
 
        usage_summary = "\n\n--- Token Usage ---"
        usage_summary += f"\nTotal Tokens: {aggregated_token_usage['total_tokens']}"
        usage_summary += f"\nPrompt Tokens: {aggregated_token_usage['prompt_tokens']}"
        usage_summary += f"\nCompletion Tokens: {aggregated_token_usage['completion_tokens']}"
        usage_summary += f"\nTotal Cost (USD): ${format(aggregated_token_usage['total_cost'], '.6f')}"
 
        return output_code + usage_summary
 
    except Exception as e:
        return f"[ERROR] {str(e)}"
 
# === Gradio UI ===
chatbot_ui = gr.ChatInterface(
    fn=process_java_project,
    title="JUnit Test Generator ",
    description=(
        "Give me a path to generate junit "
    ),
    theme="default"
)
 
if __name__ == "__main__":
    chatbot_ui.launch()





generate junit for C:\Users\rdamera\Downloads\osif-java-cg-candidate

[INFO] No parsable Java classes found.






