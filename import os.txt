import json
import requests
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import LLMChain
from langchain.agents import Tool, initialize_agent
from langchain.memory import ConversationBufferMemory

# === MCP Server Configuration ===
MCP_URL = "https://your.mcp.server.url"  # replace with actual URL

# === Azure OpenAI Client Setup ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        deployment_name="Fourth_Chatbot",
        openai_api_key="",  # insert key
        openai_api_version="2024-08-01-preview",
        openai_api_type="azure",
        temperature=0.5,
        max_tokens=1500,
        model_kwargs={"top_p": 0.9}
    )

# === Tool 1: Call MCP and Get Data ===
def call_mcp_server(_: str) -> str:
    payload = {
        "method": "tools/call",
        "params": {
            "name": "get-vendors",
            "arguments": {}
        }
    }
    try:
        response = requests.post(MCP_URL, json=payload)
        response.raise_for_status()
        data = response.json()
        return json.dumps(data, indent=2)
    except Exception as e:
        return f"[ERROR] MCP call failed: {str(e)}"

# === Tool 2: Chunk the JSON Text ===
def chunk_response_data(text: str) -> list[str]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        separators=["\n", " ", ","]
    )
    return splitter.split_text(text)

# === Tool 3: Summarize a Chunk ===
def summarize_chunk(chunk: str) -> str:
    llm = get_openai_client()
    prompt = PromptTemplate(
        input_variables=["chunk"],
        template="""
You are a data analyst assistant. Summarize the following data chunk in plain English.
Be concise, but highlight any vendors, configurations, or actions being taken.

Data:
{chunk}
"""
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    return chain.run({"chunk": chunk})

# === Tool Wrappers for LangChain Agent ===
def fetch_and_chunk(_: str) -> str:
    raw_data = call_mcp_server("")
    if raw_data.startswith("[ERROR]"):
        return raw_data
    chunks = chunk_response_data(raw_data)
    return f"[SUCCESS] MCP data fetched and split into {len(chunks)} chunks."

def summarize_all_chunks(_: str) -> str:
    raw_data = call_mcp_server("")
    if raw_data.startswith("[ERROR]"):
        return raw_data
    chunks = chunk_response_data(raw_data)
    summaries = [summarize_chunk(c) for c in chunks]
    return "\n\n".join(summaries)

# === LangChain Agent Setup ===
def run_agent():
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    tools = [
        Tool(name="FetchAndChunkMCPData", func=fetch_and_chunk, description="Fetch and chunk the MCP server response."),
        Tool(name="SummarizeChunks", func=summarize_all_chunks, description="Summarize all the MCP data chunks."),
    ]
    agent = initialize_agent(
        tools=tools,
        llm=get_openai_client(),
        agent_type="openai-functions",
        memory=memory,
        verbose=True
    )
    return agent

# === MAIN ===
if __name__ == "__main__":
    agent = run_agent()
    print("\n>> MCP Summarization Agent Running...\n")
    result = agent.invoke("Fetch data from the MCP server and summarize all results.")
    print("\n=== FINAL SUMMARY ===\n")
    print(result)
