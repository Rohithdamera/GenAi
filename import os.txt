import json
import http.client
from urllib.parse import urlparse
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.agents import Tool, initialize_agent
from langchain.memory import ConversationBufferMemory

# === MCP Configuration ===
MCP_URL = "https://your.mcp.server.url"  # âœ… Replace with actual working MCP endpoint
MCP_PAYLOAD = {
    "method": "tools/call",
    "params": {
        "name": "get-vendors",
        "arguments": {}
    }
}
MCP_HEADERS = {
    "Content-Type": "application/json"
}

# === Global Store ===
mcp_raw_data = ""

# === Azure OpenAI Setup ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        deployment_name="Fourth_Chatbot",
        openai_api_key="",  # ðŸ”‘ Add key
        openai_api_version="2024-08-01-preview",
        openai_api_type="azure",
        temperature=0.5,
        max_tokens=1500,
        model_kwargs={"top_p": 0.9}
    )

# === TOOL 1: Custom MCP Fetch using raw HTTP ===
def fetch_and_store_mcp_data(_: str) -> str:
    global mcp_raw_data
    try:
        print("â³ Connecting to MCP with raw HTTP...")

        # Parse URL parts
        parsed = urlparse(MCP_URL)
        conn = http.client.HTTPSConnection(parsed.hostname, parsed.port or 443, timeout=300)

        # Prepare path
        path = parsed.path or "/"

        # Encode body
        body = json.dumps(MCP_PAYLOAD)

        # Send request
        conn.request("POST", path, body=body, headers=MCP_HEADERS)

        # Read response
        response = conn.getresponse()
        if response.status != 200:
            return f"[ERROR] MCP server responded with status code: {response.status}"

        raw_bytes = response.read()  # Fully read the stream
        raw_text = raw_bytes.decode("utf-8")

        # Try parsing to JSON
        mcp_raw_data = json.dumps(json.loads(raw_text), indent=2)
        return "[SUCCESS] MCP data fetched and stored."

    except Exception as e:
        return f"[ERROR] Raw MCP fetch failed: {str(e)}"

# === TOOL 2: Summarize JSON chunks ===
def summarize_mcp_data(_: str) -> str:
    global mcp_raw_data
    if not mcp_raw_data:
        return "[SKIP] MCP data not fetched. Run fetch first."

    try:
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split_text(mcp_raw_data)

        client = get_openai_client()
        prompt = PromptTemplate(
            input_variables=["chunk"],
            template="""
You are an assistant. Summarize the following vendor JSON data.

- List vendor names
- Mention any fields, IDs, or relationships
- Keep it simple and readable

Chunk:
{chunk}
"""
        )
        chain = LLMChain(llm=client, prompt=prompt)
        summaries = [chain.run({"chunk": c}) for c in chunks]

        return "\n\n".join(summaries)
    except Exception as e:
        return f"[ERROR] Failed to summarize MCP data: {str(e)}"

# === LangChain Agent Setup ===
def run_agent():
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    tools = [
        Tool(name="FetchMCPData", func=fetch_and_store_mcp_data, description="Fetch vendor data using raw HTTP."),
        Tool(name="SummarizeMCPData", func=summarize_mcp_data, description="Summarize fetched vendor data.")
    ]
    return initialize_agent(
        tools=tools,
        llm=get_openai_client(),
        agent_type="openai-functions",
        memory=memory,
        verbose=True
    )

# === MAIN ===
if __name__ == "__main__":
    print("ðŸ”§ MCP Raw Agent Starting...")
    agent = run_agent()
    print("\nðŸš€ Running full pipeline...\n")
    result = agent.invoke("Fetch vendor data from MCP and summarize it.")
    print("\nâœ… Final Summary Output:\n")
    print(result)
