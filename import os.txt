import os
import json
import re
from pathlib import Path
from typing import List, Dict
import gradio as gr

from langchain.prompts import PromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain.tools import Tool
from langchain.callbacks import get_openai_callback

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        deployment_name="gpt-4_complex_conversions",
        openai_api_key=os.getenv("AZURE_OPENAI_API_KEY", ""),  # recommend using env var
        openai_api_version="2024-08-01-preview",
        openai_api_type="azure",
        temperature=0.3,
        max_tokens=2000,
    )

llm = get_openai_client()

# === Tool 1: List Java Files ===
def list_java_files(path: str) -> List[Path]:
    base_path = Path(path)
    if not base_path.exists():
        raise FileNotFoundError(f"Path not found: {path}")
    return list(base_path.rglob("*.java"))

# === Tool 2: Extract Code Info ===
def extract_code_info(java_paths: List[Path]) -> List[Dict]:
    """
    Extracts structured metadata from Java source files.
    Returns JSON with class_name, class_type, methods, uses_repository, entity_fields (if applicable).
    """

    prompt = PromptTemplate(
        input_variables=["source"],
        template="""
Return ONLY compact JSON describing this Java class with these keys:
- class_name
- class_type (must be exactly: "Controller", "Service", "Repository", "Config", "Entity", "DTO", "Main")
- package_path
- methods: list of all public method names
- uses_repository: true/false
If class_type == "Entity", also include:
  "entity_fields": list of {{"name": fieldName, "type": fieldType}}

Rules:
- Do not explain.
- Do not invent other class_type values.
- Always valid JSON.

Java Source:
{source}
"""
    )

    chain = prompt | llm
    parsed = []

    for file_path in java_paths:
        try:
            source = file_path.read_text(encoding="utf-8")
            response = chain.invoke({"source": source})
            data = json.loads(response.content.strip())
            parsed.append(data)
        except Exception as e:
            print(f"[WARN] Failed to parse {file_path}: {e}")
    return parsed

# === Tool 3: Generate JUnit Tests ===
def generate_junit_tests(parsed_info: List[Dict]) -> List[Dict]:
    ALLOWED_TYPES = {"Controller", "Service"}

    entity_map = {
        item["class_name"]: item.get("entity_fields", [])
        for item in parsed_info
        if item.get("class_type") == "Entity"
    }

    prompt = PromptTemplate(
        input_variables=[
            "class_name",
            "class_type",
            "package_path",
            "methods",
            "uses_repository",
            "entity_map",
        ],
        template="""
Write a complete JUnit 5 test class for:

Class Name: {class_name}
Type: {class_type}
Package: {package_path}
Public Methods:
{methods}
uses_repository: {uses_repository}

Available Entities:
{entity_map}

Rules:

1. Controllers
   - Use @WebMvcTest, MockMvc, @MockBean, ObjectMapper.
   - Test all endpoints, mock services, assert JSON with jsonPath.
   - Check all fields, not just IDs.

2. Services
   - Use Mockito if repository exists (@ExtendWith, @Mock, @InjectMocks).
   - Otherwise instantiate directly.
   - Populate full entities in @BeforeEach.
   - Assert with assertThat for all fields.

3. Entity population
   - Strings → "John Doe"
   - UUID → UUID.randomUUID().toString()
   - Lists → Arrays.asList("Java","Spring")
   - LocalDateTime → "2025-07-25T10:00:00"
   - int → 123
   - boolean → true

4. General
   - One @Test per public method.
   - Valid compilable Java code only.
"""
    )

    chain = prompt | llm
    test_classes = []

    for item in parsed_info:
        if item["class_type"] not in ALLOWED_TYPES:
            print(f"[SKIP] {item['class_name']} ({item['class_type']})")
            continue
        try:
            result = chain.invoke({
                "class_name": item["class_name"],
                "class_type": item["class_type"],
                "package_path": item["package_path"],
                "methods": "\n".join(item["methods"]),
                "uses_repository": str(item.get("uses_repository", False)).lower(),
                "entity_map": json.dumps(entity_map, indent=2),
            })
            test_classes.append({
                "file_name": f"{item['class_name']}Test.java",
                "package_path": item["package_path"],
                "code": result.content.strip(),
            })
        except Exception as e:
            print(f"[WARN] Could not generate test for {item['class_name']}: {e}")
    return test_classes

# === Tool 4: Save & Print JUnit Tests ===
def save_and_print_tests(junit_tests: List[Dict]) -> str:
    output = ""
    for test in junit_tests:
        folder = Path("generated_tests") / test["package_path"].replace(".", "/")
        folder.mkdir(parents=True, exist_ok=True)
        file_path = folder / test["file_name"]
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(test["code"])
        output += f"\n===== {test['file_name']} =====\n{test['code']}\n===== End of {test['file_name']} =====\n"
    return output.strip()

# === Register Tools ===
tools = [
    Tool(name="ListJavaFiles", func=list_java_files, description="Lists all Java files."),
    Tool(name="ExtractCodeInfo", func=extract_code_info, description="Parses each Java file."),
    Tool(name="GenerateJUnitTests", func=generate_junit_tests, description="Generates JUnit 5 test classes."),
    Tool(name="SaveJUnitTests", func=save_and_print_tests, description="Saves tests to disk and prints them."),
]

# === Orchestration ===
def process_java_project(user_input: str, history):
    aggregated_token_usage = {"total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0, "total_cost": 0.0}

    try:
        path_match = re.search(r"([A-Za-z]:[\\/\w\-. ]+)", user_input)
        if not path_match:
            return "[ERROR] Please provide a valid Windows file path."

        project_path = path_match.group(1).strip()

        with get_openai_callback() as cb:
            java_files = tools[0].func(project_path)
            if not java_files:
                return "[INFO] No Java files found."

            parsed_info = tools[1].func(java_files)
            if not parsed_info:
                return "[INFO] No parsable Java classes found."

            junit_tests = tools[2].func(parsed_info)
            if not junit_tests:
                return "[INFO] No JUnit tests could be generated for allowed types."

            output_code = tools[3].func(junit_tests)

            aggregated_token_usage["total_tokens"] += cb.total_tokens
            aggregated_token_usage["prompt_tokens"] += cb.prompt_tokens
            aggregated_token_usage["completion_tokens"] += cb.completion_tokens
            aggregated_token_usage["total_cost"] += cb.total_cost

        usage_summary = "\n\n--- Token Usage ---"
        usage_summary += f"\nTotal Tokens: {aggregated_token_usage['total_tokens']}"
        usage_summary += f"\nPrompt Tokens: {aggregated_token_usage['prompt_tokens']}"
        usage_summary += f"\nCompletion Tokens: {aggregated_token_usage['completion_tokens']}"
        usage_summary += f"\nTotal Cost (USD): ${format(aggregated_token_usage['total_cost'], '.6f')}"

        return output_code + usage_summary

    except Exception as e:
        return f"[ERROR] {str(e)}"

# === Gradio UI ===
chatbot_ui = gr.ChatInterface(
    fn=process_java_project,
    title="JUnit Test Generator",
    description="Provide a path to generate JUnit tests.",
    theme="default",
)

if __name__ == "__main__":
    chatbot_ui.launch()
