import os
import logging
import json
from base64 import b64decode
from Crypto.Cipher import AES
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def unpad(data):
    padding_length = data[-1]
    return data[:-padding_length]

def decrypt(data, key):
    cipher = AES.new(b64decode(key), AES.MODE_ECB)
    decrypted_data = cipher.decrypt(b64decode(data))
    return unpad(decrypted_data).decode()

def get_openai_client(model_instance_name):
    aes_key_base64 = os.environ['AES_KEY']
    decrypted_api_base = decrypt(os.environ['ENCRYPTED_API_BASE'], aes_key_base64)
    decrypted_api_key = decrypt(os.environ['ENCRYPTED_API_KEY'], aes_key_base64)
    api_version = os.environ['AZURE_API_VERSION']

    if not decrypted_api_base.endswith('/'):
        decrypted_api_base += '/'

    return AzureChatOpenAI(
        deployment_name=model_instance_name,
        openai_api_base=decrypted_api_base,
        openai_api_key=decrypted_api_key,
        openai_api_version=api_version
    )

def get_prompt(file_content):
    instruction = (
        "You are an expert in analysing the RAML and generating the payload from the RAML. "
        "I am providing the RAML which will be used as an asset for designing my API. "
        "Please anlayze the RAML and generate the sample payload which will honour all the rules inside the RAML. "
        "You can refer the link for any doubts related to RAML\n"
        "https://raml.org/developers/raml-100-tutorial\n\n"
    )
    prompt_text = "Please generate the payloads for all the endpoint in the RAML"
    full_prompt = f"{instruction}\n{prompt_text}\n\nRAML content:\n{file_content}\n\nOnly return JSON payloads for the endpoints. No explanations."
    return full_prompt

def parse_response_as_json(response_content):
    try:
        return json.loads(response_content)
    except json.JSONDecodeError:
        cleaned = response_content.strip().strip("```json").strip("```").strip()
        return json.loads(cleaned)

def process_with_openai(client, file_content, count):
    results = []
    prompt = get_prompt(file_content)
    for i in range(count):
        response = client.invoke([HumanMessage(content=prompt)])
        parsed = parse_response_as_json(response.content)
        results.append(parsed)
    return results if count > 1 else results[0]

def lambda_handler(event, context):
    try:
        logger.info(f"Received event: {json.dumps(event)}")

        if 'body' not in event or not event.get('isBase64Encoded', False):
            raise ValueError("Missing or invalid file content.")

        # Decode RAML content
        file_content = b64decode(event['body']).decode('utf-8')

        headers = event.get("headers", {}) or {}

        input_param = headers.get("input")
        model_instance_name = headers.get("model_instance_name")
        count = int(headers.get("count", 1))

        if input_param != "/sampleforraml":
            raise ValueError("Invalid input param. expected 'input =/sampleforraml'.")

        if not model_instance_name:
            raise ValueError("Missing 'model_instance_name' parameter.")

        client = get_openai_client(model_instance_name)
        result = process_with_openai(client, file_content, count)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps(result, indent=2)
        }

    except Exception as e:
        logger.error(f"Processing failed: {e}")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
