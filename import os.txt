
import os
import yaml
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
import gradio as gr
import boto3
import mysql.connector

from langchain.prompts import PromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain.tools import Tool
from langchain.callbacks import get_openai_callback
from langchain_core.tracers.stdout import ConsoleCallbackHandler
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.prebuilt import create_react_agent

# ====================================================
# Azure OpenAI Client (from code1; kept unchanged)
# ====================================================
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        deployment_name="gpt-4_complex_conversions",
        openai_api_key="",
        openai_api_version="2025-01-01-preview",
        openai_api_type="azure",
        temperature=0.3,
        max_tokens=2000,
    )

llm = get_openai_client()

# ====================================================
# Centralized Token Tracker
# ====================================================
class TokenTracker:
    """
    Centralized token tracking wrapper. Use TokenTracker.run(callable, *args, **kwargs)
    to invoke LLM functions / agent invokes while collecting token usage summary.
    """
    @staticmethod
    def run(func, *args, **kwargs):
        aggregated_token_usage = {
            "total_tokens": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_cost": 0.0,
        }

        with get_openai_callback() as cb:
            result = func(*args, **kwargs)
            # cb fields are numeric; aggregate into our structure
            aggregated_token_usage["total_tokens"] += getattr(cb, "total_tokens", 0)
            aggregated_token_usage["prompt_tokens"] += getattr(cb, "prompt_tokens", 0)
            aggregated_token_usage["completion_tokens"] += getattr(cb, "completion_tokens", 0)
            aggregated_token_usage["total_cost"] += getattr(cb, "total_cost", 0.0)

        usage_summary = "\n\n--- Token Usage ---"
        usage_summary += f"\nTotal Tokens: {aggregated_token_usage['total_tokens']}"
        usage_summary += f"\nPrompt Tokens: {aggregated_token_usage['prompt_tokens']}"
        usage_summary += f"\nCompletion Tokens: {aggregated_token_usage['completion_tokens']}"
        usage_summary += f"\nTotal Cost (USD): ${format(aggregated_token_usage['total_cost'], '.6f')}"

        return result, usage_summary

# ====================================================
# Agent 1: YAML Model Class Generator (unchanged content/prompt)
# ====================================================
def generate_model_from_yaml(prompt: str, yaml_file) -> str:
    try:
        with open(yaml_file.name, "r", encoding="utf-8") as f:
            yaml_text = f.read()
        yaml.safe_load(yaml_text)

        full_prompt = (
            "You are a senior Java developer. A developer will provide you with a YAML configuration file. "
            "Your task is to generate Java model classes only for the fields that are explicitly marked or structured with class paths. "
            "Each class path (e.g., `com.example.config.MyConfig`) should be used as the fully qualified name of the class. "
            "Use nested classes or separate classes as needed based on the YAML structure. "
            "Use lombok annotations to generate getters, setters and constructors. "
            "Do not include explanations or comments. Output only the Java class code. "
            "Whenever a class is end use this line ----------- and give a line break."
            "Give the conclusion like which classes are created classes. "
            f"\n\nPrompt: {prompt}\n\nHere is the YAML:\n\n{yaml_text}"
        )

        def _call_llm():
            return llm.invoke(full_prompt)

        response, usage = TokenTracker.run(_call_llm)
        # response is an LLM response object from AzureChatOpenAI; original code appended usage directly
        # Keep the same behavior:
        return response.content.strip() + usage

    except Exception as e:
        return f"[ERROR] Failed to process YAML: {str(e)}"

class ModelAgent:
    def run(self, prompt, yaml_file):
        return generate_model_from_yaml(prompt, yaml_file)

# ====================================================
# Agent 2: JUnit Generator (preserve all prompts and logic)
# ====================================================
def list_java_files(path: str) -> List[Path]:
    base_path = Path(path)
    if not base_path.exists():
        raise FileNotFoundError(f"Path not found: {path}")
    return list(base_path.rglob("*.java"))

def extract_code_info(java_paths: List[Path]) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["source"],
        template="""
Return ONLY compact JSON describing this Java class with these keys:
class_name
class_type (must be exactly: "Controller", "Service", "Repository", "Config", "Entity", "DTO", "Main")
package_path
methods: list of all public method names
uses_repository: true/false
If class_type == "Entity", also include:
"entity_fields": list of {{"name": fieldName, "type": fieldType}}
Rules:
Do not explain.
Do not invent other class_type values.
Always valid JSON.
Java Source:
{source}
"""
    )
    chain = prompt | llm
    parsed = []

    for file_path in java_paths:
        try:
            source = file_path.read_text(encoding="utf-8")
            response = chain.invoke({"source": source})
            data = json.loads(response.content.strip())
            parsed.append(data)
        except Exception as e:
            print(f"[WARN] Failed to parse {file_path}: {e}")
    return parsed

def generate_junit_tests(parsed_info: List[Dict]) -> List[Dict]:
    ALLOWED_TYPES = {"Controller", "Service"}
    entity_map = {item["class_name"]: item.get("entity_fields", []) for item in parsed_info if item.get("class_type") == "Entity"}
    prompt = PromptTemplate(
        input_variables=[
            "class_name", "class_type", "package_path", "methods", "uses_repository", "entity_map"
        ],
        template="""
Write a complete JUnit 5 test class for:
Class Name: {class_name}
Type: {class_type}
Package: {package_path}
Public Methods:
{methods}
uses_repository: {uses_repository}
Available Entities:
{entity_map}
Rules:
1. Controllers
Use @WebMvcTest, MockMvc, @MockBean, ObjectMapper.
Test all endpoints, mock services, assert JSON with jsonPath.
Check all fields, not just IDs.
2. Services
Use Mockito if repository exists (@ExtendWith, @Mock, @InjectMocks).
Otherwise instantiate directly.
Populate full entities in @BeforeEach.
Assert with assertThat for all fields.
3. Entity population
Strings → "John Doe"
UUID → UUID.randomUUID().toString()
Lists → Arrays.asList("Java","Spring")
LocalDateTime → "2025-07-25T10:00:00"
int → 123
boolean → true
4. General
One @Test per public method.
Valid compilable Java code only.
"""
    )
    chain = prompt | llm
    test_classes = []

    for item in parsed_info:
        if item["class_type"] not in ALLOWED_TYPES:
            print(f"[SKIP] {item['class_name']} ({item['class_type']})")
            continue
        try:
            result = chain.invoke({
                "class_name": item["class_name"],
                "class_type": item["class_type"],
                "package_path": item["package_path"],
                "methods": "\n".join(item["methods"]),
                "uses_repository": str(item.get("uses_repository", False)).lower(),
                "entity_map": json.dumps(entity_map, indent=2),
            })
            test_classes.append({
                "file_name": f"{item['class_name']}Test.java",
                "package_path": item["package_path"],
                "code": result.content.strip(),
            })
        except Exception as e:
            print(f"[WARN] Could not generate test for {item['class_name']}: {e}")
    return test_classes

def save_and_print_tests(junit_tests: List[Dict]) -> str:
    output = ""
    for test in junit_tests:
        folder = Path("generated_tests") / test["package_path"].replace(".", "/")
        folder.mkdir(parents=True, exist_ok=True)
        file_path = folder / test["file_name"]
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(test["code"])
        output += f"\n===== {test['file_name']} =====\n{test['code']}\n===== End of {test['file_name']} =====\n"
    return output.strip()

tools = [
    Tool(name="ListJavaFiles", func=list_java_files, description="Lists all Java files."),
    Tool(name="ExtractCodeInfo", func=extract_code_info, description="Parses each Java file."),
    Tool(name="GenerateJUnitTests", func=generate_junit_tests, description="Generates JUnit 5 test classes."),
    Tool(name="SaveJUnitTests", func=save_and_print_tests, description="Saves tests to disk and prints them."),
]

def process_java_project(user_input: str) -> str:
    try:
        path_match = re.search(r"([A-Za-z]:[\\/\w\-. ]+)", user_input)
        if not path_match:
            return "[ERROR] Please provide a valid Windows file path."
        project_path = path_match.group(1).strip()

        def _process():
            java_files = tools[0].func(project_path)
            if not java_files:
                return "[INFO] No Java files found."
            parsed_info = tools[1].func(java_files)
            if not parsed_info:
                return "[INFO] No parsable Java classes found."
            junit_tests = tools[2].func(parsed_info)
            if not junit_tests:
                return "[INFO] No JUnit tests could be generated for allowed types."
            return tools[3].func(junit_tests)

        output_code, usage_summary = TokenTracker.run(_process)
        return output_code + usage_summary
    except Exception as e:
        return f"[ERROR] {str(e)}"

class JUnitAgent:
    def run(self, user_input):
        return process_java_project(user_input)

# ====================================================
# Agent 3: Connector Detection (S3 lookup) — unchanged logic
# ====================================================
s3 = boto3.client(
    "s3",
    aws_access_key_id="AKIA6ODVATHCN52UKSRUO",
    aws_secret_access_key="N3+vw0XS4ZcdzqM0Zk6qflR7UbNy0ztQgdwWoiuEO",
    region_name="us-east-1"
)

S3_BUCKET = "osif-files"
S3_KEY = "OSIF_Dependency_req.txt"

def load_s3_file():
    obj = s3.get_object(Bucket=S3_BUCKET, Key=S3_KEY)
    return obj["Body"].read().decode("utf-8")

def extract_block(content, query):
    query = query.lower()
    match = re.search(r"(sales[- ]?force|sqs|crossaccountsqs|s3|common-api-library)", query, re.IGNORECASE)
    if not match:
        return "Could not identify connector from your question."

    connector_name = match.group(1).replace(" ", "").lower()
    blocks = re.split(r"(?=Title\s*:)", content, flags=re.IGNORECASE)

    for block in blocks:
        if not block.strip():
            continue
        if connector_name in block.lower().replace(" ", ""):
            return block.strip()

    return f"Connector '{connector_name}' not found in file."

class ConnectorAgent:
    def run(self, message: str) -> str:
        try:
            content = load_s3_file()
            return extract_block(content, message)
        except Exception as e:
            return f"[ERROR] {str(e)}"

# ====================================================
# Agent 4: Java Client Class Generator (unchanged)
# ====================================================
class ClientClassAgent:
    def run(self, prompt: str, java_file) -> str:
        try:
            with open(java_file.name, "r", encoding="utf-8") as f:
                java_code = f.read()
            full_prompt = (
                "You are a senior Java developer. A developer will provide you with a Java class. "
                "Analyze the java code and create client class for that. Use constructor injection and annotations also. "
                "Include the method which is there in the Java class it prints access token and instance URL, and handles exceptions. "
                "Do not include explanations or comments. Output only the Java class code. "
                f"\n\nPrompt: {prompt}\n\nHere is the Java class:\n\n{java_code}"
            )
            def _call():
                return llm.invoke(full_prompt)
            response, usage = TokenTracker.run(_call)
            return response.content.strip() + usage
        except Exception as e:
            return f"[ERROR] Failed to process Java file: {str(e)}"

# ====================================================
# Merged additions from code2:
#  - MySQL log fetching
#  - ReAct agent for file/code analysis
# ====================================================

# MySQL config (default values here — replace with env vars or secret mgmt as needed)
MYSQL_CONFIG = {
    "host": os.environ.get("MYSQL_HOST", "34.224.108.184"),
    "user": os.environ.get("MYSQL_USER", "OSIF"),
    "password": os.environ.get("MYSQL_PASSWORD", "12345@Cap"),
    "database": os.environ.get("MYSQL_DATABASE", "OSIF"),
    "port": int(os.environ.get("MYSQL_PORT", 3306)),
}

def fetch_logs_from_mysql(limit: int = 100, where_clause: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Fetch logs from MySQL. where_clause can be provided to filter specific errors/warnings.
    This function will attempt to connect using MYSQL_CONFIG.
    """
    conn = None
    results = []
    try:
        conn = mysql.connector.connect(
            host=MYSQL_CONFIG["host"],
            user=MYSQL_CONFIG["user"],
            password=MYSQL_CONFIG["password"],
            database=MYSQL_CONFIG["database"],
            port=MYSQL_CONFIG.get("port", 3306),
        )
        cursor = conn.cursor(dictionary=True)
        sql = "SELECT * FROM log_entries"
        if where_clause:
            sql += f" WHERE {where_clause}"
        sql += f" ORDER BY id DESC LIMIT {limit}"
        cursor.execute(sql)
        results = cursor.fetchall()
    except Exception as e:
        results = [{"error": str(e)}]
    finally:
        if conn:
            try:
                cursor.close()
                conn.close()
            except Exception:
                pass
    return results

# ReAct agent (from code2) using llm from code1
react_agent = create_react_agent(
    model=llm,
    tools=[],
    verbose=True
)

def invoke_react_agent_with_file(user_prompt: str, history, file_obj) -> str:
    """
    Prepare messages and invoke the ReAct agent. Wrap invocation with TokenTracker for usage.
    """
    messages = []
    if history is not None:
        for user_msg, bot_msg in history:
            messages.append(HumanMessage(content=user_msg))
            messages.append(AIMessage(content=bot_msg))

    file_content = ""
    if file_obj is not None:
        try:
            with open(file_obj.name, "r", encoding="utf-8", errors="ignore") as f:
                file_content = f.read()
        except Exception as e:
            file_content = f"[Error reading file: {e}]"

    if file_content:
        user_message = (
            f"User question:\n{user_prompt}\n\n"
            f"Attached file content:\n{file_content}\n"
        )
    else:
        user_message = user_prompt

    messages.append(HumanMessage(content=user_message))

    def _invoke():
        return react_agent.invoke(
            {"messages": messages},
            config={"callbacks": [ConsoleCallbackHandler()]}
        )

    response, usage = TokenTracker.run(_invoke)
    # response could be an LLM/agent response; try to return content if present
    try:
        return response.content.strip() + usage
    except Exception:
        # Fallback: str(response)
        return str(response) + usage

# ====================================================
# Unified Chatbot Function (merges both chat_with_agent and unified_chatbot logic)
# - Keeps code1's behavior intact, and calls ReAct / MySQL when input indicates.
# ====================================================
model_agent = ModelAgent()
junit_agent = JUnitAgent()
connector_agent = ConnectorAgent()
client_agent = ClientClassAgent()

# Keywords to detect logs and code-analysis requests
LOG_KEYWORDS = ["log", "logs", "error", "errors", "warn", "warning", "exception", "stacktrace", "traceback", "logfile"]
CODE_KEYWORDS = ["code", "function", "error in", "fix", "class", "python", "java", "file", "compile", "bug", "exception"]

def unified_chatbot(prompt, history, uploaded_files=None):
    """
    Note: uploaded_files may be:
      - None
      - a single file-like object
      - a list of file-like objects (when file_count='multiple' and user uploads multiple files)
    We map those to yaml_file / java_file to preserve existing code expectations.
    The function now also handles:
      - MySQL log fetching when AI detects log-related intent in the prompt.
      - ReAct agent-based code/file analysis when file is uploaded or code-related keywords are present.
    """
    lower_prompt = (prompt or "").lower()

    # Normalize uploaded_files into yaml_file / java_file variables expected by downstream logic
    yaml_file = None
    java_file = None
    single_file_for_react = None
    if uploaded_files is not None:
        uploaded_list = uploaded_files if isinstance(uploaded_files, list) else [uploaded_files]
        for f in uploaded_list:
            try:
                suffix = Path(f.name).suffix.lower()
            except Exception:
                continue
            if suffix in (".yml", ".yaml") and yaml_file is None:
                yaml_file = f
            elif suffix == ".java" and java_file is None:
                java_file = f
            # Keep first file for react/code analysis as well
            if single_file_for_react is None:
                single_file_for_react = f
            # If both found, break early
            if yaml_file is not None and java_file is not None:
                break

    # 1) Log detection & retrieval (AI-driven)
    if any(k in lower_prompt for k in LOG_KEYWORDS):
        # If the user included DB connection info in the prompt (not recommended in plaintext),
        # try to extract a simple where-clause or connection overrides (basic support).
        # For safety, do not parse secrets; just allow a where clause like "where message like '%NullPointer%'" if explicitly provided.
        where_clause = None
        limit = 100
        # simple extraction: look for "limit N"
        limit_match = re.search(r"limit\s+(\d{1,4})", prompt, re.IGNORECASE)
        if limit_match:
            try:
                limit = int(limit_match.group(1))
            except Exception:
                limit = 100

        # try to find a where clause in user's prompt after the word 'where' (basic and naive)
        where_match = re.search(r"where\s+(.+)", prompt, re.IGNORECASE)
        if where_match:
            raw = where_match.group(1).strip()
            # limit the length for safety
            where_clause = raw[:1000]

        try:
            logs = fetch_logs_from_mysql(limit=limit, where_clause=where_clause)
            # Format logs nicely for display
            if not logs:
                return "[INFO] No logs found."
            # If first element contains 'error' key, pass that message
            formatted = ""
            for row in logs:
                formatted += json.dumps(row, default=str) + "\n"
            return formatted
        except Exception as e:
            return f"[ERROR] Failed to fetch logs: {e}"

    # 2) Code / file analysis via ReAct agent (from code2) or JUnit flow in code1
    # If file uploaded or code keywords present => prefer ReAct agent
    if single_file_for_react is not None or any(k in lower_prompt for k in CODE_KEYWORDS):
        # If the user explicitly wants JUnit (preserve code1 behavior)
        if "junit" in lower_prompt:
       