import asyncio
import json
import nest_asyncio
from tabulate import tabulate

from mcp import ClientSession
from mcp.client.sse import sse_client

from langgraph.graph import StateGraph
from langchain.chat_models import AzureChatOpenAI

nest_asyncio.apply()

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",
        temperature=0.7,
        max_tokens=2000,
        model_kwargs={"top_p": 0.9, "frequency_penalty": 0.2, "presence_penalty": 0.1}
    )

# === SSE URL ===
sse_url = "https://employee-mcp-v1-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

# === Sync Wrapper for Async Tool Fetch ===
def sync_fetch_tools():
    async def fetch():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.list_tools()
    return asyncio.get_event_loop().run_until_complete(fetch())

# === LangGraph Node: List Available Tools ===
def list_tools_node(state: dict) -> dict:
    try:
        result = sync_fetch_tools()
        tools = [{
            "name": t.name,
            "description": t.description,
            "inputSchema": t.inputSchema
        } for t in getattr(result, "tools", [])]
        state["available_tools"] = tools
        state["__next__"] = "router"
        return state
    except Exception as e:
        return {"result": f"[ERROR] Failed to fetch tools: {e}"}

# === LLM Helpers ===
def classify_tool(user_input: str, tools: list) -> str:
    llm = get_openai_client()
    tool_list = "\n".join(f"- {t['name']}: {t['description']}" for t in tools)
    prompt = f"""You are a tool router. Choose the best tool for the input.
Available tools:

{tool_list}
User input: "{user_input}"
Return only the tool name."""
    try:
        return llm.invoke(prompt).content.strip()
    except Exception as e:
        print("Tool classification failed:", e)
        return ""

def extract_tool_input(user_input: str, schema: dict) -> dict:
    llm = get_openai_client()
    schema_desc = "\n".join(f"- {k}: {v}" for k, v in schema.get("properties", schema).items())
    prompt = f"""Extract input values for the tool based on schema.
Schema:

{schema_desc}
User input: "{user_input}"
Return JSON."""
    try:
        return json.loads(llm.invoke(prompt).content)
    except Exception as e:
        print("Input extraction failed:", e)
        return {}

def normalize_input(tool_input: dict, schema: dict) -> dict:
    keys = schema.get("properties", schema).keys()
    return {k: tool_input.get(k, "") for k in keys}

# === Sync Wrapper for Tool Execution ===
def sync_call_tool(tool_name: str, tool_input: dict):
    async def call():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.call_tool(tool_name, tool_input)
    try:
        return asyncio.get_event_loop().run_until_complete(call())
    except Exception as e:
        return f"[ERROR] {e}"

# === LangGraph Nodes ===
def router_node(state: dict) -> dict:
    tools = state.get("available_tools", [])
    tool_name = classify_tool(state["user_input"], tools)
    state["tool_name"] = tool_name
    state["__next__"] = "extract_input" if tool_name in [t["name"] for t in tools] else "default"
    return state

def extract_input_node(state: dict) -> dict:
    tools = state.get("available_tools", [])
    tool = next((t for t in tools if t["name"] == state["tool_name"]), None)
    if not tool:
        return {"result": "[ERROR] Tool not found."}
    raw_input = extract_tool_input(state["user_input"], tool["inputSchema"])
    normalized = normalize_input(raw_input, tool["inputSchema"])
    state.update({
        "tool_input": normalized,
        "schema_display": json.dumps(tool["inputSchema"].get("properties", {}), indent=2),
        "input_display": json.dumps(normalized, indent=2),
        "__next__": "call_tool"
    })
    return state

def call_tool_node(state: dict) -> dict:
    result = sync_call_tool(state["tool_name"], state["tool_input"])

    json_data = None
    if hasattr(result, "content") and result.content:
        for item in result.content:
            if hasattr(item, "text"):
                try:
                    json_data = json.loads(item.text)
                    break
                except Exception:
                    continue

    tool_used_msg = f"\n[INFO] Tool used: {state['tool_name']}\n"

    if isinstance(json_data, list) and json_data and all(isinstance(row, dict) for row in json_data):
        headers = json_data[0].keys()
        rows = [list(row.values()) for row in json_data]
        table = tabulate(rows, headers=headers, tablefmt="grid")
        return {"result": f"{table}{tool_used_msg}"}
    elif isinstance(result, str) and result.startswith("[ERROR]"):
        return {"result": f"{result}{tool_used_msg}"}
    else:
        return {"result": f"[INFO] Tool executed but returned no structured data.\nRaw Output: {result}{tool_used_msg}"}

def default_node(state: dict) -> dict:
    return {"result": "No matching tool found."}

# === Build LangGraph ===
def build_graph():
    graph = StateGraph(dict)
    graph.add_node("list_tools", list_tools_node)
    graph.add_node("router", router_node)
    graph.add_node("extract_input", extract_input_node)
    graph.add_node("call_tool", call_tool_node)
    graph.add_node("default", default_node)

    graph.add_conditional_edges("list_tools", lambda s: s["__next__"], {
        "router": "router"
    })
    graph.add_conditional_edges("router", lambda s: s["__next__"], {
        "extract_input": "extract_input", "default": "default"
    })
    graph.add_conditional_edges("extract_input", lambda s: s["__next__"], {
        "call_tool": "call_tool"
    })

    graph.set_entry_point("list_tools")
    return graph.compile()

# === Main Execution ===
if __name__ == "__main__":
    initial_state = {}
    tool_state = list_tools_node(initial_state)

    if "available_tools" not in tool_state:
        print(tool_state.get("result", "[ERROR] Could not load tools."))
        exit()

    print("\nAvailable Tools:")
    for t in tool_state["available_tools"]:
        print(f"- {t['name']}: {t['description']}")

    user_input = input("\nDescribe the tool you want to use: ")
    tool_state["user_input"] = user_input

    runnable = build_graph()
    result = runnable.invoke(tool_state)

    print("\nTool Output:\n", result.get("result", "No result returned."))
