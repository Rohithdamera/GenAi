import os
import json
import logging
from base64 import b64decode
from Crypto.Cipher import AES
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

logging.basicConfig(level=logging.INFO)

def unpad(data):
    return data[:-data[-1]]

def decrypt(data, key):
    cipher = AES.new(b64decode(key), AES.MODE_ECB)
    decrypted = cipher.decrypt(b64decode(data))
    return unpad(decrypted).decode()

def initialize_openai():
    try:
        aes_key = os.environ['AES_KEY']
        api_base = decrypt(os.environ['ENCRYPTED_API_BASE'], aes_key)
        api_key = decrypt(os.environ['ENCRYPTED_API_KEY'], aes_key)
        version = os.environ['AZURE_API_VERSION']

        return AzureChatOpenAI(
            deployment_name=os.environ.get('MODEL_NAME', 'Default_Model'),
            openai_api_base=api_base.rstrip('/') + '/',
            openai_api_key=api_key,
            openai_api_version=version,
            temperature=0.0,
            max_tokens=4096
        )
    except Exception as e:
        logging.error(f"OpenAI init error: {e}")
        raise

def detect_file_type(content: str) -> str:
    if content.strip().startswith("#%RAML"):
        return "raml"
    try:
        data = json.loads(content)
        if "swagger" in data or "openapi" in data:
            return "swagger"
    except json.JSONDecodeError:
        pass
    raise ValueError("Unsupported or unrecognized file format.")

def get_prompt(file_type: str) -> str:
    if file_type == "raml":
        return "Generate API test cases using Postman syntax from the given RAML file content."
    elif file_type == "swagger":
        return "Generate API test cases using Postman syntax from the given Swagger/OpenAPI JSON."
    else:
        raise ValueError("Unknown file type for prompt generation.")

def generate_test_cases(client, content, file_type):
    prompt = get_prompt(file_type)
    messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=content)
    ]
    result = client.invoke(messages)
    return result.content.strip()

def lambda_handler(event, context):
    try:
        if 'body' not in event or not event['body']:
            raise ValueError("Missing file content in request body.")

        file_content = (
            b64decode(event['body']).decode('utf-8')
            if event.get('isBase64Encoded', False)
            else event['body']
        )

        file_type = detect_file_type(file_content)
        client = initialize_openai()
        test_cases = generate_test_cases(client, file_content, file_type)

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "text/plain"},
            "body": test_cases
        }

    except Exception as e:
        logging.error(f"Handler error: {e}")
        return {
            "statusCode": 400,
            "body": json.dumps({"error": str(e)})
        }
