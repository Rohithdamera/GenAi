import gradio as gr
import mysql.connector
import json
from langchain_openai import AzureChatOpenAI
from langchain.callbacks import get_openai_callback
from langchain_core.tracers.stdout import ConsoleCallbackHandler
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage, AIMessage
from langchain.callbacks.tracers import ConsoleCallbackHandler as ConsoleHandler2
from langchain_community.callbacks import get_openai_callback as get_openai_callback_2


# -------------------------------
# Token Usage Handler (Single Class)
# -------------------------------
class TokenUsageHandler:
    @staticmethod
    def track_and_invoke(func, *args, **kwargs):
        """Wraps LLM/Agent calls with token usage tracking."""
        with get_openai_callback() as cb:
            result = func(*args, **kwargs)
            usage_summary = (
                "\n\n--- Token Usage ---"
                f"\nTotal Tokens: {cb.total_tokens}"
                f"\nPrompt Tokens: {cb.prompt_tokens}"
                f"\nCompletion Tokens: {cb.completion_tokens}"
                f"\nTotal Cost (USD): ${format(cb.total_cost, '.6f')}"
            )
        return result, usage_summary


# -------------------------------
# Configure Azure OpenAI (shared)
# -------------------------------
llm = AzureChatOpenAI(
    deployment_name="gpt-4_complex_conversions",
    azure_endpoint="https://testopenaiassets.openai.azure.com",
    openai_api_key="",
    openai_api_version="2025-01-01-preview"
)

# -------------------------------
# Configure MySQL (Code 1 part)
# -------------------------------
config = {
    "host": "34.224.108.183",
    "user": "OSIF",
    "password": "123456@Cap",
    "database": "OSIF"
}


def fetch_logs():
    """Fetch logs or API performance data from MySQL."""
    conn = mysql.connector.connect(**config)
    cursor = conn.cursor(dictionary=True)

    try:
        cursor.execute("SELECT * FROM log_entries ORDER BY id DESC LIMIT 100")
        results = cursor.fetchall()
    except Exception as e:
        results = [{"error": str(e)}]

    cursor.close()
    conn.close()
    return results


# -------------------------------
# Chat Function (Code 1)
# -------------------------------
def chat_with_agent_logs(user_prompt, history):
    try:
        results = fetch_logs()

        if not results or "error" in results[0]:
            return f"Could not fetch logs from DB: {results}"

        # Create log summary
        level_counts = {}
        for log in results:
            level = log.get("level", "UNKNOWN").upper()
            level_counts[level] = level_counts.get(level, 0) + 1

        summary = "\n".join([f"{level}: {count}" for level, count in level_counts.items()])
        debug_count = level_counts.get("DEBUG", 0)

        # Logs to JSON
        logs_str = json.dumps(results, indent=2, default=str)

        # Construct final prompt
        messages = [
            {
                "role": "system",
                "content": (
                    """
You are a highly experienced data analyst specializing in system and API log analysis.

Your task is to analyze structured log data provided to you.
Instructions:
- Always base your answers strictly on the provided log data.
- If the requested information is not present in the logs, clearly state that it is unavailable.
- Avoid assumptions.
- Provide concise, structured, and insightful responses.
"""
                ),
            },
            {
                "role": "user",
                "content": (
                    f"{user_prompt}\n\n"
                    f"--- Log Summary ---\n{summary}\n\n"
                    f"Total DEBUG logs: {debug_count}\n\n"
                    f"--- Raw Logs (latest 100 rows) ---\n{logs_str}"
                ),
            },
        ]

        # Call LLM with shared token handler
        def invoke_llm():
            return llm.invoke(messages, config={"callbacks": [ConsoleCallbackHandler()]})

        result, usage_summary = TokenUsageHandler.track_and_invoke(invoke_llm)
        final_response = result.content + usage_summary
        return final_response

    except Exception as e:
        return f"Error: {str(e)}"


# -------------------------------
# Agent Function (Code 2)
# -------------------------------
agent = create_react_agent(
    model=llm,
    tools=[],
    verbose=True
)


def chat_with_agent_files(message, history, file=None):
    messages = []

    # Ensure history exists
    if history is not None:
        for user_msg, bot_msg in history:
            messages.append(HumanMessage(content=user_msg))
            messages.append(AIMessage(content=bot_msg))

    # If a file is uploaded
    file_content = ""
    if file is not None:
        try:
            with open(file.name, "r", encoding="utf-8", errors="ignore") as f:
                file_content = f.read()
        except Exception as e:
            file_content = f"[Error reading file: {e}]"

    # Construct user message
    if file_content:
        user_message = (
            f"User question:\n{message}\n\n"
            f"Attached file content:\n{file_content}\n"
        )
    else:
        user_message = message

    messages.append(HumanMessage(content=user_message))

    # Call agent with shared token handler
    def invoke_agent():
        return agent.invoke({"messages": messages}, config={"callbacks": [ConsoleHandler2()]})

    result, usage_summary = TokenUsageHandler.track_and_invoke(invoke_agent)
    final_response = result["messages"][-1].content + usage_summary
    return final_response


# -------------------------------
# Gradio Interfaces
# -------------------------------
chatbot_logs = gr.ChatInterface(
    fn=chat_with_agent_logs,
    title="OSIF Co-Developer (Logs)",
    description="Query system/API logs with natural language",
    theme="default"
)

chatbot_files = gr.ChatInterface(
    fn=chat_with_agent_files,
    additional_inputs=[
        gr.File(
            label="Upload a file (optional)",
            file_types=[".txt", ".java", ".py", ".js", ".cpp", ".md", ".log", "*"]
        )
    ],
    title="OSIF Co-Developer (Files)",
    description="Ask coding or log-related questions, paste code, or upload files.",
    theme="default"
)


# -------------------------------
# Launch both UIs
# -------------------------------
if __name__ == "__main__":
    chatbot_logs.launch(debug=False, share=False)
    chatbot_files.launch(debug=False, share=False)