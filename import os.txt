import os
import logging
import json
from base64 import b64decode
from Crypto.Cipher import AES
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def unpad(data):
    """Remove PKCS7 padding from decrypted data."""
    padding_length = data[-1]
    return data[:-padding_length]

def decrypt(data, key):
    """Decrypt base64 encoded AES ECB encrypted data."""
    cipher = AES.new(b64decode(key), AES.MODE_ECB)
    decrypted_data = cipher.decrypt(b64decode(data))
    decrypted_data = unpad(decrypted_data)
    return decrypted_data.decode()

def get_openai_client(model_instance_name):
    """Initialize Azure OpenAI client using decrypted credentials from env vars."""
    try:
        aes_key_base64 = os.environ['AES_KEY']
        encrypted_api_base = os.environ['ENCRYPTED_API_BASE']
        encrypted_api_key = os.environ['ENCRYPTED_API_KEY']
        api_version = os.environ['AZURE_API_VERSION']

        decrypted_api_base = decrypt(encrypted_api_base, aes_key_base64)
        decrypted_api_key = decrypt(encrypted_api_key, aes_key_base64)

        if not decrypted_api_base.endswith('/'):
            decrypted_api_base += '/'

        request_url = f"{decrypted_api_base}openai/deployments/{model_instance_name}/chat/completions?api-version={api_version}"
        logger.info(f"Constructed Request URL: {request_url}")

        client = AzureChatOpenAI(
            deployment_name=model_instance_name,
            openai_api_base=decrypted_api_base,
            openai_api_key=decrypted_api_key,
            openai_api_version=api_version
        )
        return client
    except Exception as e:
        logger.error(f"Error initializing OpenAI client: {e}")
        raise ValueError(f"Error initializing OpenAI client: {e}")

def process_with_openai(client, file_content):
    """Process RAML content with the prompt and call OpenAI."""
    try:
        instructions = (
            "You are an expert in analysing the RAML and generating the payload from the RAML. "
            "I am providing the RAML which will be used as an asset for designing my API. "
            "Please analyze the RAML and generate the sample payload which will honor all the rules inside the RAML. "
            "You can refer the link for any doubts related to RAML https://raml.org/developers/raml-100-tutorial"
        )
        prompt = (
            instructions + "\n\nPlease generate the payloads for all the endpoints in the RAML."
        )
        # Call OpenAI client with the prompt + RAML content
        response = client.invoke([HumanMessage(content=prompt + "\n\n" + file_content)])
        return response.content
    except Exception as e:
        logger.error(f"Error in OpenAI API call: {e}")
        raise

def lambda_handler(event, context):
    import traceback
    logger.info(f"Event keys: {list(event.keys())}")
    logger.info(f"isBase64Encoded: {event.get('isBase64Encoded')}")
    logger.info(f"Headers: {event.get('headers')}")
    logger.info(f"Query params: {event.get('queryStringParameters')}")

    try:
        # Validate body presence
        if 'body' not in event or not event['body']:
            raise ValueError("Missing file content in request body")

        # Decode body if base64 encoded
        if event.get('isBase64Encoded', False):
            logger.info("Body is base64 encoded; decoding")
            file_content = b64decode(event['body']).decode('utf-8')
        else:
            logger.info("Body is plain text")
            file_content = event['body']

        logger.info(f"File content preview: {file_content[:200]}")

        # Validate RAML file signature
        if not (file_content.strip().startswith("#%RAML") or file_content.strip().startswith("# RAML")):
            raise ValueError("Invalid RAML file format. Expected a RAML starting with '#%RAML'.")

        # Validate query param 'input'
        query_params = event.get('queryStringParameters', {})
        input_param = query_params.get('input')
        if not input_param:
            raise ValueError("Missing 'input' query parameter")
        if input_param != "sampleforraml":
            raise ValueError(f"Unsupported input param: {input_param}. Expected 'sampleforraml'.")

        # Validate model_instance_name in headers
        headers = event.get('headers', {})
        model_instance_name = headers.get('model_instance_name')
        if not model_instance_name:
            raise ValueError("Missing 'model_instance_name' header")

        logger.info(f"Using model_instance_name: {model_instance_name}")

        # Initialize OpenAI client
        client = get_openai_client(model_instance_name)

        # Process file content with OpenAI
        response = process_with_openai(client, file_content)

        # Return response
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": response
        }
    except Exception as e:
        tb = traceback.format_exc()
        logger.error(f"Exception caught: {e}\n{tb}")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
