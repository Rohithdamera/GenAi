import os
import logging
import json
from base64 import b64decode
from Crypto.Cipher import AES
from langchain_community.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Helper to unpad decrypted data
def unpad(data):
    padding_length = data[-1]
    return data[:-padding_length]

# AES Decryptor
def decrypt(data, key):
    cipher = AES.new(b64decode(key), AES.MODE_ECB)
    decrypted_data = cipher.decrypt(b64decode(data))
    decrypted_data = unpad(decrypted_data)
    return decrypted_data.decode()

# Get OpenAI client
def get_openai_client(model_instance_name):
    try:
        aes_key_base64 = os.environ['AES_KEY']
        encrypted_api_base = os.environ['ENCRYPTED_API_BASE']
        encrypted_api_key = os.environ['ENCRYPTED_API_KEY']
        api_version = os.environ['AZURE_API_VERSION']

        decrypted_api_base = decrypt(encrypted_api_base, aes_key_base64)
        decrypted_api_key = decrypt(encrypted_api_key, aes_key_base64)

        if not decrypted_api_base.endswith('/'):
            decrypted_api_base += '/'

        return AzureChatOpenAI(
            deployment_name=model_instance_name,
            openai_api_base=decrypted_api_base,
            openai_api_key=decrypted_api_key,
            openai_api_version=api_version
        )
    except Exception as e:
        logger.error(f"OpenAI client init error: {e}")
        raise ValueError(f"OpenAI client init error: {e}")

# Detect content type
def detect_file_type(content: str):
    if content.strip().startswith("<?xml") or "<xs:schema" in content:
        return "xsd"
    elif "raml_version" in content or "#%RAML" in content:
        return "yml"
    return "unknown"

# Get instructions and prompt
def get_instruction_and_prompt(input_value, file_type):
    if input_value == "/sampleforraml":
        if file_type != "yml":
            raise ValueError("Only /sampleforraml command is allowed for .yml (RAML) files.")
        instruction = (
            "You are an expert in analysing the RAML and generating the payload from the RAML. "
            "I am providing the RAML which will be used as an asset for designing my API. "
            "Please anlayze the RAML and generate the sample payload which will honour all the rules inside the RAML. "
            "You can refer the link for any doubts related to RAML\n"
            "https://raml.org/developers/raml-100-tutorial"
        )
        prompt = "Please generate the payloads for all the endpoint in the RAML."
        content_type = "application/json"

    elif input_value == "/sampleforxsd":
        if file_type != "xsd":
            raise ValueError("Only /sampleforxsd command is allowed for .xsd files.")
        instruction = (
            "You are an expert in analysing the XSD and generating the payload from the XSD. "
            "I am providing the XSD which will be used as an asset for designing my API. "
            "Please anlayze the XSD and generate the sample payload which will honour all the rules inside the XSD. "
            "You can refer the link for any doubts related to XSD\n"
            "https://www.tutorialspoint.com/xsd/index.htm"
        )
        prompt = "Please generate the payloads for the given XSD."
        content_type = "application/xml"

    else:
        raise ValueError(f"Unsupported input value: {input_value}")

    return instruction, prompt, content_type

# Lambda handler
def lambda_handler(event, context):
    logger.info(f"Incoming event: {json.dumps(event)[:500]}")

    try:
        if 'body' not in event or not event['isBase64Encoded']:
            raise ValueError("File content is missing or not base64-encoded.")

        file_bytes = b64decode(event['body'])
        file_content = file_bytes.decode('utf-8')

        headers = {k.lower(): v for k, v in (event.get('headers') or {}).items()}
        query_params = event.get('queryStringParameters', {}) or {}

        input_value = query_params.get('input')
        if not input_value:
            raise ValueError("Missing 'input' query parameter.")

        model_instance_name = headers.get('model_instance_name')
        if not model_instance_name:
            raise ValueError("Missing 'model_instance_name' header.")

        count = int(headers.get('count', '1'))

        # Detect type and validate
        detected_type = detect_file_type(file_content)
        if detected_type == "unknown":
            raise ValueError("Unable to detect file type. Ensure it's a valid .yml or .xsd file.")

        instruction, prompt, content_type = get_instruction_and_prompt(input_value, detected_type)
        client = get_openai_client(model_instance_name)

        responses = []
        for i in range(count):
            logger.info(f"Generating payload {i+1} of {count}")
            combined_prompt = f"{instruction}\n\n{prompt}\n\n{file_content}"
            result = client.invoke([HumanMessage(content=combined_prompt)])
            responses.append(result.content.strip())

        if input_value == "/sampleforraml":
            return {
                "statusCode": 200,
                "headers": {"Content-Type": content_type},
                "body": json.dumps(responses, indent=2)
            }
        else:
            return {
                "statusCode": 200,
                "headers": {"Content-Type": content_type},
                "body": "\n\n".join(responses)
            }

    except Exception as e:
        logger.error(f"Error: {e}")
        return {
            "statusCode": 400,
            "body": json.dumps({"error": str(e)})
        }
