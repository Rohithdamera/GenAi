import asyncio
import json
import nest_asyncio
from tabulate import tabulate

from langgraph.graph import StateGraph
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
import aiohttp

nest_asyncio.apply()

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",  # Fill in your key
        temperature=0.7,
        max_tokens=2000,
        model_kwargs={"top_p": 0.9, "frequency_penalty": 0.2, "presence_penalty": 0.1}
    )

# === SSE URL ===
sse_url = "https://employee-mcp-v1-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

# === Async Tool Fetch ===
async def fetch_tools():
    payload = {"method": "tools/list", "params": {}}
    async with aiohttp.ClientSession() as session:
        async with session.post(sse_url, json=payload) as resp:
            try:
                data = await resp.json()
                return data.get("tools", [])
            except:
                return []

# === Async Tool Caller ===
async def call_tool(tool_name, tool_input):
    payload = {
        "method": "tools/call",
        "params": {
            "name": tool_name,
            "arguments": tool_input
        }
    }
    async with aiohttp.ClientSession() as session:
        async with session.post(sse_url, json=payload) as resp:
            try:
                return await resp.json()
            except:
                return {"error": "Invalid response or no data."}

# === LangGraph Nodes ===
def decide_and_call_node(state: dict) -> dict:
    user_input = state["user_input"]
    tools = asyncio.run(fetch_tools())

    tool_list_str = "\n".join(f"- {t['name']}: {t['description']}" for t in tools)
    prompt = f"""
You are an intelligent agent that decides which tool(s) to use based on user input.
Tools available:
{tool_list_str}

User query:
"{user_input}"

Decide and return a JSON list like:
[
  {{"tool": "ToolName", "input": {{...}}}},
  ...
]
If no tools are relevant, return: [NO DATA FOUND]
"""

    try:
        llm = get_openai_client()
        plan = llm.invoke(HumanMessage(content=prompt)).content.strip()

        if "[NO DATA FOUND]" in plan:
            print("\n[Agent]: No relevant tools found for the query.")
            state["__next__"] = "ask_next"
            return state

        try:
            tool_calls = json.loads(plan)
        except:
            print("\n[Agent]: Invalid tool plan generated by LLM.")
            state["__next__"] = "ask_next"
            return state

        results = []
        for call in tool_calls:
            tool = call.get("tool")
            args = call.get("input", {})
            output = asyncio.run(call_tool(tool, args))
            results.append({"tool": tool, "output": output})

        response_prompt = f"""
User asked: {user_input}
Tool results:
{json.dumps(results, indent=2)}

Respond clearly and completely to the user based on these results.
"""
        final_answer = llm.invoke(HumanMessage(content=response_prompt)).content
        print("\n[Agent]:", final_answer)
    except Exception as e:
        print(f"\n[ERROR]: {e}")

    state["__next__"] = "ask_next"
    return state

def ask_next_node(state: dict) -> dict:
    user_input = input("\nYou: ").strip()
    if user_input.lower() == "exit":
        state["exit"] = True
        state["__next__"] = "exit"
    else:
        state["user_input"] = user_input
        state["__next__"] = "decide_and_call"
    return state

def exit_node(state: dict) -> dict:
    print("\n[INFO] Session ended. Goodbye!")
    return state

# === Graph ===
def build_graph():
    graph = StateGraph(dict)
    graph.add_node("ask_next", ask_next_node)
    graph.add_node("decide_and_call", decide_and_call_node)
    graph.add_node("exit", exit_node)

    graph.set_entry_point("ask_next")
    graph.add_conditional_edges("ask_next", lambda s: s["__next__"], {
        "decide_and_call": "decide_and_call",
        "exit": "exit"
    })
    graph.add_conditional_edges("decide_and_call", lambda s: s["__next__"], {
        "ask_next": "ask_next"
    })

    return graph.compile()

# === Main ===
if __name__ == "__main__":
    print("\n[Agent Ready] Ask any question related to employees or projects. Type 'exit' to stop.")
    graph = build_graph()
    state = {}
    while True:
        state = graph.invoke(state)
        if state.get("exit"):
            break
