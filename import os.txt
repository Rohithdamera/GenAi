import asyncio
import json
import nest_asyncio
from mcp import ClientSession
from mcp.client.sse import sse_client
from langchain.chat_models import AzureChatOpenAI
from langchain.agents import tool
from langgraph.prebuilt import create_plan_and_execute_agent

nest_asyncio.apply()

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",  # <-- Add your actual API key or load from env
        temperature=0.7,
        max_tokens=2000,
        model_kwargs={
            "top_p": 0.9,
            "frequency_penalty": 0.2,
            "presence_penalty": 0.1
        }
    )

# === SSE Stream URL ===
sse_url = "https://employee-mcp-v1-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

# === Tool Call Execution ===
def sync_call_tool(tool_name: str, tool_input: dict):
    async def call():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.call_tool(tool_name, tool_input)
    try:
        return asyncio.get_event_loop().run_until_complete(call())
    except Exception as e:
        return f"[ERROR] {e}"

# === Dynamic Tool Loader from MCP Server ===
def sync_fetch_tools():
    async def fetch():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                tools_data = await session.list_tools()
                tools = []

                for t in getattr(tools_data, "tools", []):
                    tool_name = t.name
                    tool_description = t.description

                    # Define a tool dynamically using langchain.agent.tool
                    @tool(name=tool_name, description=tool_description)
                    def _tool(input_data: dict, tool_name=tool_name):
                        return sync_call_tool(tool_name, input_data)

                    tools.append(_tool)
                return tools
    return asyncio.get_event_loop().run_until_complete(fetch())

# === Build Plan & Execute Agent ===
def build_plan_and_execute_graph():
    llm = get_openai_client()
    tools = sync_fetch_tools()

    if not tools:
        raise RuntimeError("No tools available from MCP Server.")

    # Build a plan-and-execute agent from LangGraph
    agent_executor = create_plan_and_execute_agent(
        llm=llm,
        tools=tools,
        verbose=True
    )

    return agent_executor

# === Main Interactive CLI ===
if __name__ == "__main__":
    print("\n=== MCP Tool Assistant (Plan & Execute Agent) ===\nType 'exit' to quit.\n")

    try:
        runnable = build_plan_and_execute_graph()
    except Exception as e:
        print(f"[ERROR] Failed to initialize agent: {e}")
        exit(1)

    while True:
        user_input = input("Ask your question: ").strip()
        if user_input.lower() in ("exit", "quit"):
            print("Goodbye!")
            break

        try:
            result = runnable.invoke({"input": user_input})
            print("\nResponse:\n", result.get("output", "[No result returned.]"))
        except Exception as e:
            print(f"[ERROR] Failed to process query: {e}")
