
import asyncio
import json
import nest_asyncio
from tabulate import tabulate
from mcp import ClientSession
from mcp.client.sse import sse_client
from langgraph.graph import StateGraph
from langchain.chat_models import AzureChatOpenAI

nest_asyncio.apply()

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_deployment="Fourth_Chatbot",
        azure_endpoint="https://testopenaiassets.openai.azure.com",
        openai_api_version="2024-08-01-preview",
        openai_api_key="",  # Add your key via env
        temperature=0.7,
        max_tokens=2000,
        model_kwargs={"top_p": 0.9, "frequency_penalty": 0.2, "presence_penalty": 0.1}
    )

# === SSE URL ===
sse_url = "https://employee-mcp-v1-6b0n6.dw4w1g-2.gbr-e1.cloudhub.io/sse"

# === Sync Wrapper for Async Tool Fetch ===
def sync_fetch_tools():
    async def fetch():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.list_tools()
    return asyncio.get_event_loop().run_until_complete(fetch())

# === Sync Tool Execution ===
def sync_call_tool(tool_name: str, tool_input: dict):
    async def call():
        async with sse_client(url=sse_url) as streams:
            async with ClientSession(*streams) as session:
                await session.initialize()
                return await session.call_tool(tool_name, tool_input)
    try:
        return asyncio.get_event_loop().run_until_complete(call())
    except Exception as e:
        return f"[ERROR] {e}"

# === Node: List All Tools ===
def list_tools_node(state: dict) -> dict:
    try:
        result = sync_fetch_tools()
        tools = [{
            "name": t.name,
            "description": t.description,
            "inputSchema": t.inputSchema
        } for t in getattr(result, "tools", [])]
        state["available_tools"] = tools
        state["__next__"] = "plan_tools"
        return state
    except Exception as e:
        return {"result": f"[ERROR] Failed to fetch tools: {e}"}

# === LLM Reasoning ===
def plan_tool_calls(user_input: str, tools: list) -> list:
    llm = get_openai_client()
    tool_list = "\n".join([f"{t['name']}: {t['description']}" for t in tools])
    prompt = f"""
You are a planner agent. The user asked:

"{user_input}"

Here are available tools:

{tool_list}

Decide which tools should be used to fulfill the user's request. 
Return a JSON list of tool call plan in the following format:

[
  {{
    "tool_name": "ToolName",
    "purpose": "Why this tool is needed"
  }},
  ...
]
"""
    try:
        response = llm.invoke(prompt).content
        return json.loads(response)
    except Exception as e:
        print("[Plan Error]", e)
        return []

# === Extract Input for a Tool ===
def extract_tool_input(user_input: str, tool_name: str, schema: dict) -> dict:
    llm = get_openai_client()
    schema_str = "\n".join(f"- {k}: {v}" for k, v in schema.get("properties", {}).items())
    prompt = f"""Given this user input: "{user_input}" 
and the following tool schema for {tool_name}:

{schema_str}

Extract the JSON input that matches the schema.
Only return the JSON."""
    try:
        response = llm.invoke(prompt).content
        return json.loads(response)
    except Exception as e:
        print("Input extract failed:", e)
        return {}

# === Execute Tool Calls One by One ===
def execute_tool_plan(state: dict) -> dict:
    tools = state.get("available_tools", [])
    user_input = state.get("user_input", "")
    plan = plan_tool_calls(user_input, tools)
    
    results = []

    for step in plan:
        tool_name = step["tool_name"]
        tool = next((t for t in tools if t["name"] == tool_name), None)
        if not tool:
            results.append(f"[WARN] Tool '{tool_name}' not found.")
            continue
        input_data = extract_tool_input(user_input, tool_name, tool["inputSchema"])
        output = sync_call_tool(tool_name, input_data)

        # Format output (try parsing json if response is structured)
        formatted = None
        if hasattr(output, "content"):
            for part in output.content:
                if hasattr(part, "text"):
                    try:
                        parsed = json.loads(part.text)
                        if isinstance(parsed, list) and all(isinstance(row, dict) for row in parsed):
                            headers = parsed[0].keys()
                            rows = [list(r.values()) for r in parsed]
                            formatted = tabulate(rows, headers, tablefmt="grid")
                        else:
                            formatted = json.dumps(parsed, indent=2)
                        break
                    except Exception:
                        continue
        results.append(f"[{tool_name} Result]\n{formatted or str(output)}")

    return {"result": "\n\n".join(results)}

# === Fallback ===
def fallback_node(state: dict) -> dict:
    return {"result": "[INFO] Could not find a matching plan or tool."}

# === LangGraph ===
def build_graph():
    graph = StateGraph(dict)
    graph.add_node("list_tools", list_tools_node)
    graph.add_node("plan_tools", execute_tool_plan)
    graph.add_node("default", fallback_node)

    graph.add_conditional_edges("list_tools", lambda s: s["__next__"], {
        "plan_tools": "plan_tools"
    })

    graph.set_entry_point("list_tools")
    return graph.compile()

# === CLI Entry ===
if __name__ == "__main__":
    initial_state = {}
    tool_state = list_tools_node(initial_state)

    if "available_tools" not in tool_state:
        print(tool_state.get("result", "[ERROR] Could not load tools."))
        exit()

    print("\nAvailable Tools:")
    for t in tool_state["available_tools"]:
        print(f"- {t['name']}: {t['description']}")

    user_input = input("\nDescribe what you want: ")
    tool_state["user_input"] = user_input

    runnable = build_graph()
    result = runnable.invoke(tool_state)

    print("\nOutput:\n", result.get("result", "No result returned."))

