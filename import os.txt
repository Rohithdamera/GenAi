import os
import json
import re
from pathlib import Path
from typing import List, Dict, Set

import gradio as gr

from langchain.prompts import PromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain.callbacks.openai import get_openai_callback

# === Azure OpenAI Client ===
def get_openai_client():
    return AzureChatOpenAI(
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT", "https://testopenaiassets.openai.azure.com"),
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT", "Fourth_Chatbot"),
        openai_api_key=os.getenv("AZURE_OPENAI_KEY", ""),
        openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview"),
        openai_api_type="azure",
        temperature=0.3,
        max_tokens=2000,
    )

llm = get_openai_client()

# Track token usage
aggregated_token_usage = {
    "total_tokens": 0,
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_cost": 0.0
}

def accumulate_usage(cb):
    aggregated_token_usage["total_tokens"] += getattr(cb, "total_tokens", 0)
    aggregated_token_usage["prompt_tokens"] += getattr(cb, "prompt_tokens", 0)
    aggregated_token_usage["completion_tokens"] += getattr(cb, "completion_tokens", 0)
    aggregated_token_usage["total_cost"] += float(getattr(cb, "total_cost", 0.0))

# === Step 1: Find Java Files ===
def find_java_files(path: str) -> List[Path]:
    base_path = Path(path)
    if not base_path.exists():
        raise FileNotFoundError(f"Path not found: {path}")
    return list(base_path.rglob("*.java"))

# === Step 2: Find Repository Classes ===
def find_repository_classes(java_paths: List[Path]) -> Set[str]:
    repo_names = set()
    for path in java_paths:
        try:
            text = path.read_text(encoding="utf-8")
            if "interface" in text and "Repository" in text:
                class_match = re.search(r"interface\s+(\w+Repository)\b", text)
                if class_match:
                    repo_names.add(class_match.group(1))
        except Exception:
            pass
    return repo_names

# === Step 3: Extract Entity Fields ===
def extract_entity_fields(java_paths: List[Path]) -> Dict[str, List[str]]:
    """
    Returns a dict mapping entity/model class name to a list of its private variable names.
    """
    entity_fields = {}
    for path in java_paths:
        try:
            text = path.read_text(encoding="utf-8")
            if "@Entity" in text or "class" in text and "{" in text:
                class_match = re.search(r"class\s+(\w+)", text)
                if class_match:
                    class_name = class_match.group(1)
                    fields = re.findall(r"private\s+\S+\s+(\w+)\s*;", text)
                    if fields:
                        entity_fields[class_name] = fields
        except Exception:
            pass
    return entity_fields

# === Exclusion Heuristic ===
EXCLUDE_TYPES = {"Model", "Entity", "DTO", "Repository", "Config", "TestData", "Data", "Dto", "EntityClass"}

def is_excluded(file_path: Path, source: str, parsed_class_type: str = None, class_name: str = None) -> bool:
    if parsed_class_type and any(x.lower() in parsed_class_type.lower() for x in EXCLUDE_TYPES):
        return True
    if class_name and any(suffix.lower() in class_name.lower() for suffix in ["entity", "dto", "repository", "repo", "config", "testdata", "data"]):
        return True
    fname = file_path.name.lower()
    if any(term in fname for term in ["entity", "dto", "repository", "repo", "config", "testdata", "data"]):
        return True
    if "@Entity" in source or "extends JpaRepository" in source:
        return True
    return False

# === Step 4: Extract Class Metadata ===
def extract_class_metadata(java_paths: List[Path]) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["source"],
        template="""
Analyze the Java class and return JSON:
- class_name
- class_type (Controller, Service, etc.)
- package_path
- methods: list of public method names

Return only valid JSON. No commentary.
Java Source:
{source}
"""
    )
    chain = prompt | llm
    parsed = []
    for file_path in java_paths:
        try:
            source = file_path.read_text(encoding="utf-8")
            with get_openai_callback() as cb:
                response = chain.invoke({"source": source})
                accumulate_usage(cb)
            data = json.loads(response.content.strip())
            excluded = is_excluded(file_path, source, data.get("class_type"), data.get("class_name"))
            parsed.append({
                "file_path": str(file_path),
                "class_name": data.get("class_name", file_path.stem),
                "class_type": data.get("class_type", ""),
                "package_path": data.get("package_path", ""),
                "methods": data.get("methods", []),
                "_excluded": excluded
            })
        except Exception as e:
            print(f"[WARN] Failed to parse {file_path}: {e}")
    return parsed

# === Step 5: Generate JUnit Tests ===
def generate_junit_tests(parsed_info: List[Dict], repo_names: Set[str], entity_fields: Dict[str, List[str]]) -> List[Dict]:
    prompt = PromptTemplate(
        input_variables=["class_name", "class_type", "package_path", "methods", "existing_repos", "entity_fields"],
        template="""
Write a JUnit 5 test class for:
- Class: {class_name}
- Type: {class_type}
- Package: {package_path}
- Public Methods:
{methods}

Rules:
1. If {class_type} is "Service" and it uses a repository that is NOT in {existing_repos}, do NOT inject or mock it â€” instantiate the service directly.
2. For any entity/domain object used in tests, use EXACTLY these fields when creating test instances:
{entity_fields}
   - Use the setter methods for each field (no constructor).
   - Assign realistic sample values for each field.
3. For Controller: use @WebMvcTest and MockMvc; for Service without repository: no mocks, direct instance.
4. Output only valid Java code.
"""
    )
    chain = prompt | llm
    tests = []
    for item in parsed_info:
        if item["_excluded"] or not item["methods"]:
            continue

        # Match entity fields if service name suggests it handles a certain entity
        entity_info = ""
        for entity, fields in entity_fields.items():
            if entity.lower() in item["class_name"].lower():
                entity_info = f"{entity}: {', '.join(fields)}"
                break
        if not entity_info:
            entity_info = "None"

        with get_openai_callback() as cb:
            result = chain.invoke({
                "class_name": item["class_name"],
                "class_type": item["class_type"],
                "package_path": item["package_path"],
                "methods": "\n".join(item["methods"]),
                "existing_repos": ", ".join(repo_names) if repo_names else "None",
                "entity_fields": entity_info
            })
            accumulate_usage(cb)

        tests.append({
            "file_name": f"{item['class_name']}Test.java",
            "package_path": item["package_path"],
            "code": result.content.strip(),
            "source_file": item["file_path"]
        })
    return tests

# === Step 6: Save & Return Output ===
def save_and_format_output(junit_tests: List[Dict]) -> str:
    output = ""
    for test in junit_tests:
        folder = Path("generated_tests") / test["package_path"].replace(".", "/")
        folder.mkdir(parents=True, exist_ok=True)
        (folder / test["file_name"]).write_text(test["code"], encoding="utf-8")
        output += f"\n===== {test['file_name']} =====\n{test['code']}\n===== End =====\n"
    return output.strip()

# === Main ===
def process_java_project(prompt: str, history):
    global aggregated_token_usage
    aggregated_token_usage = {k: 0 for k in aggregated_token_usage}
    path_match = re.search(r"([A-Za-z]:[\\\/][\w\-. \\\/]+)|((?:/|~)[\w\-.\\/]+)", prompt)
    if not path_match:
        return "[ERROR] Please provide a valid path."
    project_path = os.path.expanduser(path_match.group(0).strip())

    java_paths = find_java_files(project_path)
    if not java_paths:
        return "[INFO] No Java files found."

    repo_names = find_repository_classes(java_paths)
    entity_fields = extract_entity_fields(java_paths)
    parsed_info = extract_class_metadata(java_paths)

    allowed = [p for p in parsed_info if not p["_excluded"]]
    if not allowed:
        return "[INFO] All classes excluded."

    tests = generate_junit_tests(allowed, repo_names, entity_fields)
    if not tests:
        return "[INFO] No tests generated."

    output = save_and_format_output(tests)
    usage = (
        f"\n\n--- Token Usage ---\n"
        f"Total Tokens: {aggregated_token_usage['total_tokens']}\n"
        f"Prompt Tokens: {aggregated_token_usage['prompt_tokens']}\n"
        f"Completion Tokens: {aggregated_token_usage['completion_tokens']}\n"
        f"Total Cost (USD): ${aggregated_token_usage['total_cost']:.6f}"
    )
    return output + usage

# === Gradio UI ===
chatbot_ui = gr.ChatInterface(
    fn=process_java_project,
    title="OSIF Co-Developer - Entity-Aware JUnit",
    description="Generates JUnit tests that populate all fields from actual entity definitions, skipping phantom repos.",
    theme="default"
)

if __name__ == "__main__":
    chatbot_ui.launch()
